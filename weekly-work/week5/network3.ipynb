{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network3.ipynb based on network3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### from http://neuralnetworksanddeeplearning.com/chap6.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle \n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv\n",
    "from theano.tensor.nnet import softmax\n",
    "from theano.tensor import shared_randomstreams\n",
    "from theano.tensor.signal import downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions for neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(z): return z\n",
    "def ReLU(z): return T.maximum(0.0, z)\n",
    "from theano.tensor.nnet import sigmoid\n",
    "from theano.tensor import tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with a CPU.  If this is not desired, then the modify network3.py to set\n",
      "the GPU flag to True.\n"
     ]
    }
   ],
   "source": [
    "GPU = False\n",
    "if GPU:\n",
    "    print \"Trying to run under a GPU.  If this is not desired, then modify \"+\\\n",
    "        \"network3.py\\nto set the GPU flag to False.\"\n",
    "    try: theano.config.device = 'gpu'\n",
    "    except: pass # it's already set\n",
    "    theano.config.floatX = 'float32'\n",
    "else:\n",
    "    print \"Running with a CPU.  If this is not desired, then the modify \"+\\\n",
    "        \"network3.py to set\\nthe GPU flag to True.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_shared(filename=\"../../neural-networks-and-deep-learning/data/mnist.pkl.gz\"):\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    def shared(data):\n",
    "        \"\"\"Place the data into shared variables.  This allows Theano to copy\n",
    "        the data to the GPU, if one is available.\n",
    "\n",
    "        \"\"\"\n",
    "        shared_x = theano.shared(\n",
    "            np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
    "        shared_y = theano.shared(\n",
    "            np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
    "        return shared_x, T.cast(shared_y, \"int32\")\n",
    "    return [shared(training_data), shared(validation_data), shared(test_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main class used to construct and train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, layers, mini_batch_size):\n",
    "        \"\"\"Takes a list of `layers`, describing the network architecture, and\n",
    "        a value for the `mini_batch_size` to be used during training\n",
    "        by stochastic gradient descent.\n",
    "\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.params = [param for layer in self.layers for param in layer.params]\n",
    "        self.x = T.matrix(\"x\")\n",
    "        self.y = T.ivector(\"y\")\n",
    "        init_layer = self.layers[0]\n",
    "        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
    "        for j in xrange(1, len(self.layers)):\n",
    "            prev_layer, layer  = self.layers[j-1], self.layers[j]\n",
    "            layer.set_inpt(\n",
    "                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
    "        self.output = self.layers[-1].output\n",
    "        self.output_dropout = self.layers[-1].output_dropout\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            validation_data, test_data, lmbda=0.0):\n",
    "        \"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
    "        training_x, training_y = training_data\n",
    "        validation_x, validation_y = validation_data\n",
    "        test_x, test_y = test_data\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        num_training_batches = size(training_data)/mini_batch_size\n",
    "        num_validation_batches = size(validation_data)/mini_batch_size\n",
    "        num_test_batches = size(test_data)/mini_batch_size\n",
    "\n",
    "        # define the (regularized) cost function, symbolic gradients, and updates\n",
    "        l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
    "        cost = self.layers[-1].cost(self)+\\\n",
    "               0.5*lmbda*l2_norm_squared/num_training_batches\n",
    "        grads = T.grad(cost, self.params)\n",
    "        updates = [(param, param-eta*grad)\n",
    "                   for param, grad in zip(self.params, grads)]\n",
    "\n",
    "        # define functions to train a mini-batch, and to compute the\n",
    "        # accuracy in validation and test mini-batches.\n",
    "        i = T.lscalar() # mini-batch index\n",
    "        train_mb = theano.function(\n",
    "            [i], cost, updates=updates,\n",
    "            givens={\n",
    "                self.x:\n",
    "                training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        validate_mb_accuracy = theano.function(\n",
    "            [i], self.layers[-1].accuracy(self.y),\n",
    "            givens={\n",
    "                self.x:\n",
    "                validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        test_mb_accuracy = theano.function(\n",
    "            [i], self.layers[-1].accuracy(self.y),\n",
    "            givens={\n",
    "                self.x:\n",
    "                test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        self.test_mb_predictions = theano.function(\n",
    "            [i], self.layers[-1].y_out,\n",
    "            givens={\n",
    "                self.x:\n",
    "                test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        # Do the actual training\n",
    "        best_validation_accuracy = 0.0\n",
    "        for epoch in xrange(epochs):\n",
    "            for minibatch_index in xrange(num_training_batches):\n",
    "                iteration = num_training_batches*epoch+minibatch_index\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(\"Training mini-batch number {0}\".format(iteration))\n",
    "                cost_ij = train_mb(minibatch_index)\n",
    "                if (iteration+1) % num_training_batches == 0:\n",
    "                    validation_accuracy = np.mean(\n",
    "                        [validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
    "                    print(\"Epoch {0}: validation accuracy {1:.2%}\".format(\n",
    "                        epoch, validation_accuracy))\n",
    "                    if validation_accuracy >= best_validation_accuracy:\n",
    "                        print(\"This is the best validation accuracy to date.\")\n",
    "                        best_validation_accuracy = validation_accuracy\n",
    "                        best_iteration = iteration\n",
    "                        if test_data:\n",
    "                            test_accuracy = np.mean(\n",
    "                                [test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
    "                            print('The corresponding test accuracy is {0:.2%}'.format(\n",
    "                                test_accuracy))\n",
    "        print(\"Finished training network.\")\n",
    "        print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
    "            best_validation_accuracy, best_iteration))\n",
    "        print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define layer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvPoolLayer(object):\n",
    "    \"\"\"Used to create a combination of a convolutional and a max-pooling\n",
    "    layer.  A more sophisticated implementation would separate the\n",
    "    two, but for our purposes we'll always use them together, and it\n",
    "    simplifies the code, so it makes sense to combine them.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filter_shape, image_shape, poolsize=(2, 2),\n",
    "                 activation_fn=sigmoid):\n",
    "        \"\"\"`filter_shape` is a tuple of length 4, whose entries are the number\n",
    "        of filters, the number of input feature maps, the filter height, and the\n",
    "        filter width.\n",
    "\n",
    "        `image_shape` is a tuple of length 4, whose entries are the\n",
    "        mini-batch size, the number of input feature maps, the image\n",
    "        height, and the image width.\n",
    "\n",
    "        `poolsize` is a tuple of length 2, whose entries are the y and\n",
    "        x pooling sizes.\n",
    "\n",
    "        \"\"\"\n",
    "        self.filter_shape = filter_shape\n",
    "        self.image_shape = image_shape\n",
    "        self.poolsize = poolsize\n",
    "        self.activation_fn=activation_fn\n",
    "        # initialize weights and biases\n",
    "        n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))\n",
    "        self.w = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape),\n",
    "                dtype=theano.config.floatX),\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
    "                dtype=theano.config.floatX),\n",
    "            borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape(self.image_shape)\n",
    "        conv_out = conv.conv2d(\n",
    "            input=self.inpt, filters=self.w, filter_shape=self.filter_shape,\n",
    "            image_shape=self.image_shape)\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out, ds=self.poolsize, ignore_border=True)\n",
    "        self.output = self.activation_fn(\n",
    "            pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "        self.output_dropout = self.output # no dropout in the convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "\n",
    "    def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.activation_fn = activation_fn\n",
    "        self.p_dropout = p_dropout\n",
    "        # Initialize weights and biases\n",
    "        self.w = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(\n",
    "                    loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
    "                dtype=theano.config.floatX),\n",
    "            name='w', borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
    "                       dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
    "        self.output = self.activation_fn(\n",
    "            (1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(\n",
    "            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
    "        self.output_dropout = self.activation_fn(\n",
    "            T.dot(self.inpt_dropout, self.w) + self.b)\n",
    "\n",
    "    def accuracy(self, y):\n",
    "        \"Return the accuracy for the mini-batch.\"\n",
    "        return T.mean(T.eq(y, self.y_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer(object):\n",
    "\n",
    "    def __init__(self, n_in, n_out, p_dropout=0.0):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.p_dropout = p_dropout\n",
    "        # Initialize weights and biases\n",
    "        self.w = theano.shared(\n",
    "            np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
    "            name='w', borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
    "        self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(\n",
    "            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
    "        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)\n",
    "\n",
    "    def cost(self, net):\n",
    "        \"Return the log-likelihood cost.\"\n",
    "        return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\n",
    "\n",
    "    def accuracy(self, y):\n",
    "        \"Return the accuracy for the mini-batch.\"\n",
    "        return T.mean(T.eq(y, self.y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellanea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def size(data):\n",
    "    \"Return the size of the dataset `data`.\"\n",
    "    return data[0].get_value(borrow=True).shape[0]\n",
    "\n",
    "def dropout_layer(layer, p_dropout):\n",
    "    srng = shared_randomstreams.RandomStreams(\n",
    "        np.random.RandomState(0).randint(999999))\n",
    "    mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
    "    return layer*T.cast(mask, theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_shared()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run shallow architecture (single hidden layer) as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([\n",
    "        FullyConnectedLayer(n_in=784, n_out=100),\n",
    "        SoftmaxLayer(n_in=100, n_out=10)\n",
    "    ], mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(Elemwise{mul,no_inplace}.0, w)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op Dot22(Elemwise{mul,no_inplace}.0, w) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(Elemwise{mul,no_inplace}.0, w)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op Dot22(Elemwise{mul,no_inplace}.0, w) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(DimShuffle{1,0}.0, SoftmaxGrad.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (DimShuffle{1,0}.0) of Op Dot22(DimShuffle{1,0}.0, SoftmaxGrad.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(SoftmaxGrad.0, w.T)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (SoftmaxGrad.0) of Op Dot22(SoftmaxGrad.0, w.T) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(DimShuffle{1,0}.0, Elemwise{mul}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (DimShuffle{1,0}.0) of Op Dot22(DimShuffle{1,0}.0, Elemwise{mul}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
      "    ret = fill_chain(new_inputs[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
      "    out = _fill_chain(v, node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
      "    new_out = T.fill(i, new_out)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{second,no_inplace}(dot.0, dot.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
      "    ret = fill_chain(new_inputs[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
      "    out = _fill_chain(v, node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
      "    new_out = T.fill(i, new_out)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{second,no_inplace}(dot.0, dot.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_advanced_indexing_crossentropy_onehot_grad\n",
      "ERROR (theano.gof.opt): node: SoftmaxGrad(Elemwise{true_div}.0, Softmax.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 1907, in local_advanced_indexing_crossentropy_onehot_grad\n",
      "    out_grad = -incr\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 39, in __neg__\n",
      "    return theano.tensor.basic.neg(self)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{second}.0) of Op Elemwise{neg,no_inplace}(Elemwise{second}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(TensorConstant{-1.0}, mean)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5427, in local_mul_specialize\n",
      "    rval = -new_inputs[0]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 39, in __neg__\n",
      "    return theano.tensor.basic.neg(self)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (mean) of Op Elemwise{neg,no_inplace}(mean) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_advanced_indexing_crossentropy_onehot\n",
      "ERROR (theano.gof.opt): node: AdvancedSubtensor(Elemwise{log,no_inplace}.0, ARange{dtype='int64'}.0, Elemwise{Cast{int32}}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 1692, in local_advanced_indexing_crossentropy_onehot\n",
      "    b_var = tensor.zeros_like(x_var[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 532, in __getitem__\n",
      "    lambda entry: isinstance(entry, Variable)))\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{add,no_inplace}.0) of Op Subtensor{int64}(Elemwise{add,no_inplace}.0, Constant{0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_logsoftmax\n",
      "ERROR (theano.gof.opt): node: Elemwise{log,no_inplace}(Softmax.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 756, in local_logsoftmax\n",
      "    ret = new_op(inVars)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{add,no_inplace}.0) of Op LogSoftmax(Elemwise{add,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): SeqOptimizer apply <theano.tensor.opt.FusionOptimizer object at 0x7f4ee231a9d0>\n",
      "ERROR (theano.gof.opt): Traceback:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 230, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 89, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 6658, in apply\n",
      "    new_outputs = self.optimizer(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 6727, in local_add_mul_fusion\n",
      "    output_node = node.op(*(l + inp.owner.inputs))\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{sub}.0) of Op Elemwise{mul}(Elemwise{sub}.0, Reshape{2}.0, sigmoid.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): SeqOptimizer apply <theano.tensor.opt.FusionOptimizer object at 0x7f4ee231aa90>\n",
      "ERROR (theano.gof.opt): Traceback:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 230, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 89, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 6658, in apply\n",
      "    new_outputs = self.optimizer(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 6502, in local_fuse\n",
      "    return_list=True)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<float64>) of Op mul(<float64>, <float64>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): SeqOptimizer apply <theano.compile.mode.AddDestroyHandler object at 0x7f4f0d7aba10>\n",
      "ERROR (theano.gof.opt): Traceback:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 230, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 89, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/compile/mode.py\", line 134, in apply\n",
      "    fgraph.replace_validate(o, _output_guard(o),\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op OutputGuard(Elemwise{mul,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0}(Elemwise{mul,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0}(Elemwise{mul,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{x}(Elemwise{Cast{float64}}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{Cast{float64}}.0) of Op InplaceDimShuffle{x}(Elemwise{Cast{float64}}.0) missing default value\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Epoch 0: validation accuracy 92.41%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 91.67%\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Epoch 1: validation accuracy 94.67%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 94.01%\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Epoch 2: validation accuracy 95.79%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 95.09%\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Epoch 3: validation accuracy 96.28%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 95.69%\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 4: validation accuracy 96.67%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.17%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Epoch 5: validation accuracy 96.87%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.51%\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Epoch 6: validation accuracy 96.95%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.75%\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Epoch 7: validation accuracy 97.10%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.05%\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Epoch 8: validation accuracy 97.30%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.22%\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 9: validation accuracy 97.40%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.31%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Epoch 10: validation accuracy 97.47%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.38%\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Epoch 11: validation accuracy 97.51%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.41%\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Epoch 12: validation accuracy 97.50%\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Epoch 13: validation accuracy 97.52%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.48%\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 14: validation accuracy 97.59%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.57%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Epoch 15: validation accuracy 97.59%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.62%\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Epoch 16: validation accuracy 97.56%\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Epoch 17: validation accuracy 97.58%\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Epoch 18: validation accuracy 97.64%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.68%\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 19: validation accuracy 97.62%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Epoch 20: validation accuracy 97.64%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.70%\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Epoch 21: validation accuracy 97.64%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.72%\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Epoch 22: validation accuracy 97.66%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.74%\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Epoch 23: validation accuracy 97.69%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.76%\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 24: validation accuracy 97.69%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.77%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Epoch 25: validation accuracy 97.69%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.80%\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Epoch 26: validation accuracy 97.67%\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Epoch 27: validation accuracy 97.69%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.80%\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Epoch 28: validation accuracy 97.71%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.81%\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 29: validation accuracy 97.70%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Epoch 30: validation accuracy 97.71%\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Epoch 31: validation accuracy 97.71%\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Epoch 32: validation accuracy 97.72%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.80%\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Epoch 33: validation accuracy 97.72%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.82%\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 34: validation accuracy 97.72%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.79%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Epoch 35: validation accuracy 97.72%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.78%\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Epoch 36: validation accuracy 97.74%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.81%\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Epoch 37: validation accuracy 97.74%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.81%\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Epoch 38: validation accuracy 97.75%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.80%\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 39: validation accuracy 97.77%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.80%\n",
      "Training mini-batch number 200000\n",
      "Training mini-batch number 201000\n",
      "Training mini-batch number 202000\n",
      "Training mini-batch number 203000\n",
      "Training mini-batch number 204000\n",
      "Epoch 40: validation accuracy 97.78%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.79%\n",
      "Training mini-batch number 205000\n",
      "Training mini-batch number 206000\n",
      "Training mini-batch number 207000\n",
      "Training mini-batch number 208000\n",
      "Training mini-batch number 209000\n",
      "Epoch 41: validation accuracy 97.77%\n",
      "Training mini-batch number 210000\n",
      "Training mini-batch number 211000\n",
      "Training mini-batch number 212000\n",
      "Training mini-batch number 213000\n",
      "Training mini-batch number 214000\n",
      "Epoch 42: validation accuracy 97.80%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.82%\n",
      "Training mini-batch number 215000\n",
      "Training mini-batch number 216000\n",
      "Training mini-batch number 217000\n",
      "Training mini-batch number 218000\n",
      "Training mini-batch number 219000\n",
      "Epoch 43: validation accuracy 97.79%\n",
      "Training mini-batch number 220000\n",
      "Training mini-batch number 221000\n",
      "Training mini-batch number 222000\n",
      "Training mini-batch number 223000\n",
      "Training mini-batch number 224000\n",
      "Epoch 44: validation accuracy 97.78%\n",
      "Training mini-batch number 225000\n",
      "Training mini-batch number 226000\n",
      "Training mini-batch number 227000\n",
      "Training mini-batch number 228000\n",
      "Training mini-batch number 229000\n",
      "Epoch 45: validation accuracy 97.79%\n",
      "Training mini-batch number 230000\n",
      "Training mini-batch number 231000\n",
      "Training mini-batch number 232000\n",
      "Training mini-batch number 233000\n",
      "Training mini-batch number 234000\n",
      "Epoch 46: validation accuracy 97.80%\n",
      "Training mini-batch number 235000\n",
      "Training mini-batch number 236000\n",
      "Training mini-batch number 237000\n",
      "Training mini-batch number 238000\n",
      "Training mini-batch number 239000\n",
      "Epoch 47: validation accuracy 97.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.82%\n",
      "Training mini-batch number 240000\n",
      "Training mini-batch number 241000\n",
      "Training mini-batch number 242000\n",
      "Training mini-batch number 243000\n",
      "Training mini-batch number 244000\n",
      "Epoch 48: validation accuracy 97.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.81%\n",
      "Training mini-batch number 245000\n",
      "Training mini-batch number 246000\n",
      "Training mini-batch number 247000\n",
      "Training mini-batch number 248000\n",
      "Training mini-batch number 249000\n",
      "Epoch 49: validation accuracy 97.79%\n",
      "Training mini-batch number 250000\n",
      "Training mini-batch number 251000\n",
      "Training mini-batch number 252000\n",
      "Training mini-batch number 253000\n",
      "Training mini-batch number 254000\n",
      "Epoch 50: validation accuracy 97.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.81%\n",
      "Training mini-batch number 255000\n",
      "Training mini-batch number 256000\n",
      "Training mini-batch number 257000\n",
      "Training mini-batch number 258000\n",
      "Training mini-batch number 259000\n",
      "Epoch 51: validation accuracy 97.80%\n",
      "Training mini-batch number 260000\n",
      "Training mini-batch number 261000\n",
      "Training mini-batch number 262000\n",
      "Training mini-batch number 263000\n",
      "Training mini-batch number 264000\n",
      "Epoch 52: validation accuracy 97.80%\n",
      "Training mini-batch number 265000\n",
      "Training mini-batch number 266000\n",
      "Training mini-batch number 267000\n",
      "Training mini-batch number 268000\n",
      "Training mini-batch number 269000\n",
      "Epoch 53: validation accuracy 97.80%\n",
      "Training mini-batch number 270000\n",
      "Training mini-batch number 271000\n",
      "Training mini-batch number 272000\n",
      "Training mini-batch number 273000\n",
      "Training mini-batch number 274000\n",
      "Epoch 54: validation accuracy 97.79%\n",
      "Training mini-batch number 275000\n",
      "Training mini-batch number 276000\n",
      "Training mini-batch number 277000\n",
      "Training mini-batch number 278000\n",
      "Training mini-batch number 279000\n",
      "Epoch 55: validation accuracy 97.80%\n",
      "Training mini-batch number 280000\n",
      "Training mini-batch number 281000\n",
      "Training mini-batch number 282000\n",
      "Training mini-batch number 283000\n",
      "Training mini-batch number 284000\n",
      "Epoch 56: validation accuracy 97.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.84%\n",
      "Training mini-batch number 285000\n",
      "Training mini-batch number 286000\n",
      "Training mini-batch number 287000\n",
      "Training mini-batch number 288000\n",
      "Training mini-batch number 289000\n",
      "Epoch 57: validation accuracy 97.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.82%\n",
      "Training mini-batch number 290000\n",
      "Training mini-batch number 291000\n",
      "Training mini-batch number 292000\n",
      "Training mini-batch number 293000\n",
      "Training mini-batch number 294000\n",
      "Epoch 58: validation accuracy 97.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.85%\n",
      "Training mini-batch number 295000\n",
      "Training mini-batch number 296000\n",
      "Training mini-batch number 297000\n",
      "Training mini-batch number 298000\n",
      "Training mini-batch number 299000\n",
      "Epoch 59: validation accuracy 97.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.85%\n",
      "Finished training network.\n",
      "Best validation accuracy of 97.81% obtained at iteration 299999\n",
      "Corresponding test accuracy of 97.85%\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a convolutional layer: \n",
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
    "                     filter_shape=(20, 1, 5, 5),\n",
    "                     poolsize=(2, 2)),\n",
    "        FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
    "        SoftmaxLayer(n_in=100, n_out=10)], \n",
    "              mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Epoch 0: validation accuracy 93.56%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 92.74%\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Epoch 1: validation accuracy 96.03%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 95.51%\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Epoch 2: validation accuracy 96.96%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.57%\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Epoch 3: validation accuracy 97.41%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.10%\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 4: validation accuracy 97.80%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.47%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Epoch 5: validation accuracy 97.99%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.73%\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Epoch 6: validation accuracy 98.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.83%\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Epoch 7: validation accuracy 98.14%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.88%\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Epoch 8: validation accuracy 98.25%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.07%\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 9: validation accuracy 98.40%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.24%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Epoch 10: validation accuracy 98.52%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.31%\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Epoch 11: validation accuracy 98.58%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.43%\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Epoch 12: validation accuracy 98.61%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.51%\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Epoch 13: validation accuracy 98.66%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.58%\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 14: validation accuracy 98.66%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.67%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Epoch 15: validation accuracy 98.68%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.69%\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Epoch 16: validation accuracy 98.69%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.76%\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Epoch 17: validation accuracy 98.68%\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Epoch 18: validation accuracy 98.71%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.74%\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 19: validation accuracy 98.71%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.74%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Epoch 20: validation accuracy 98.73%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.74%\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Epoch 21: validation accuracy 98.74%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.78%\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Epoch 22: validation accuracy 98.75%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.78%\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Epoch 23: validation accuracy 98.73%\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 24: validation accuracy 98.75%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.81%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Epoch 25: validation accuracy 98.78%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.80%\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Epoch 26: validation accuracy 98.76%\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Epoch 27: validation accuracy 98.75%\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Epoch 28: validation accuracy 98.74%\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 29: validation accuracy 98.75%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Epoch 30: validation accuracy 98.75%\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Epoch 31: validation accuracy 98.75%\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Epoch 32: validation accuracy 98.76%\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Epoch 33: validation accuracy 98.76%\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 34: validation accuracy 98.76%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Epoch 35: validation accuracy 98.75%\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Epoch 36: validation accuracy 98.75%\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Epoch 37: validation accuracy 98.75%\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Epoch 38: validation accuracy 98.76%\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 39: validation accuracy 98.75%\n",
      "Training mini-batch number 200000\n",
      "Training mini-batch number 201000\n",
      "Training mini-batch number 202000\n",
      "Training mini-batch number 203000\n",
      "Training mini-batch number 204000\n",
      "Epoch 40: validation accuracy 98.75%\n",
      "Training mini-batch number 205000\n",
      "Training mini-batch number 206000\n",
      "Training mini-batch number 207000\n",
      "Training mini-batch number 208000\n",
      "Training mini-batch number 209000\n",
      "Epoch 41: validation accuracy 98.76%\n",
      "Training mini-batch number 210000\n",
      "Training mini-batch number 211000\n",
      "Training mini-batch number 212000\n",
      "Training mini-batch number 213000\n",
      "Training mini-batch number 214000\n",
      "Epoch 42: validation accuracy 98.76%\n",
      "Training mini-batch number 215000\n",
      "Training mini-batch number 216000\n",
      "Training mini-batch number 217000\n",
      "Training mini-batch number 218000\n",
      "Training mini-batch number 219000\n",
      "Epoch 43: validation accuracy 98.77%\n",
      "Training mini-batch number 220000\n",
      "Training mini-batch number 221000\n",
      "Training mini-batch number 222000\n",
      "Training mini-batch number 223000\n",
      "Training mini-batch number 224000\n",
      "Epoch 44: validation accuracy 98.77%\n",
      "Training mini-batch number 225000\n",
      "Training mini-batch number 226000\n",
      "Training mini-batch number 227000\n",
      "Training mini-batch number 228000\n",
      "Training mini-batch number 229000\n",
      "Epoch 45: validation accuracy 98.77%\n",
      "Training mini-batch number 230000\n",
      "Training mini-batch number 231000\n",
      "Training mini-batch number 232000\n",
      "Training mini-batch number 233000\n",
      "Training mini-batch number 234000\n",
      "Epoch 46: validation accuracy 98.78%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.90%\n",
      "Training mini-batch number 235000\n",
      "Training mini-batch number 236000\n",
      "Training mini-batch number 237000\n",
      "Training mini-batch number 238000\n",
      "Training mini-batch number 239000\n",
      "Epoch 47: validation accuracy 98.79%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.90%\n",
      "Training mini-batch number 240000\n",
      "Training mini-batch number 241000\n",
      "Training mini-batch number 242000\n",
      "Training mini-batch number 243000\n",
      "Training mini-batch number 244000\n",
      "Epoch 48: validation accuracy 98.79%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.90%\n",
      "Training mini-batch number 245000\n",
      "Training mini-batch number 246000\n",
      "Training mini-batch number 247000\n",
      "Training mini-batch number 248000\n",
      "Training mini-batch number 249000\n",
      "Epoch 49: validation accuracy 98.78%\n",
      "Training mini-batch number 250000\n",
      "Training mini-batch number 251000\n",
      "Training mini-batch number 252000\n",
      "Training mini-batch number 253000\n",
      "Training mini-batch number 254000\n",
      "Epoch 50: validation accuracy 98.80%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.92%\n",
      "Training mini-batch number 255000\n",
      "Training mini-batch number 256000\n",
      "Training mini-batch number 257000\n",
      "Training mini-batch number 258000\n",
      "Training mini-batch number 259000\n",
      "Epoch 51: validation accuracy 98.80%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.93%\n",
      "Training mini-batch number 260000\n",
      "Training mini-batch number 261000\n",
      "Training mini-batch number 262000\n",
      "Training mini-batch number 263000\n",
      "Training mini-batch number 264000\n",
      "Epoch 52: validation accuracy 98.80%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.95%\n",
      "Training mini-batch number 265000\n",
      "Training mini-batch number 266000\n",
      "Training mini-batch number 267000\n",
      "Training mini-batch number 268000\n",
      "Training mini-batch number 269000\n",
      "Epoch 53: validation accuracy 98.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.95%\n",
      "Training mini-batch number 270000\n",
      "Training mini-batch number 271000\n",
      "Training mini-batch number 272000\n",
      "Training mini-batch number 273000\n",
      "Training mini-batch number 274000\n",
      "Epoch 54: validation accuracy 98.81%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.95%\n",
      "Training mini-batch number 275000\n",
      "Training mini-batch number 276000\n",
      "Training mini-batch number 277000\n",
      "Training mini-batch number 278000\n",
      "Training mini-batch number 279000\n",
      "Epoch 55: validation accuracy 98.80%\n",
      "Training mini-batch number 280000\n",
      "Training mini-batch number 281000\n",
      "Training mini-batch number 282000\n",
      "Training mini-batch number 283000\n",
      "Training mini-batch number 284000\n",
      "Epoch 56: validation accuracy 98.80%\n",
      "Training mini-batch number 285000\n",
      "Training mini-batch number 286000\n",
      "Training mini-batch number 287000\n",
      "Training mini-batch number 288000\n",
      "Training mini-batch number 289000\n",
      "Epoch 57: validation accuracy 98.82%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.95%\n",
      "Training mini-batch number 290000\n",
      "Training mini-batch number 291000\n",
      "Training mini-batch number 292000\n",
      "Training mini-batch number 293000\n",
      "Training mini-batch number 294000\n",
      "Epoch 58: validation accuracy 98.82%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.93%\n",
      "Training mini-batch number 295000\n",
      "Training mini-batch number 296000\n",
      "Training mini-batch number 297000\n",
      "Training mini-batch number 298000\n",
      "Training mini-batch number 299000\n",
      "Epoch 59: validation accuracy 98.82%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.92%\n",
      "Finished training network.\n",
      "Best validation accuracy of 98.82% obtained at iteration 299999\n",
      "Corresponding test accuracy of 98.92%\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove fully connected layer: \n",
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
    "                     filter_shape=(20, 1, 5, 5), \n",
    "                     poolsize=(2, 2)),\n",
    "        # FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
    "        SoftmaxLayer(n_in=20*12*12, n_out=10)], \n",
    "              mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Epoch 0: validation accuracy 94.05%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 93.53%\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Epoch 1: validation accuracy 96.88%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.58%\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Epoch 2: validation accuracy 97.62%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.52%\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Epoch 3: validation accuracy 98.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.80%\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 4: validation accuracy 98.16%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.99%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Epoch 5: validation accuracy 98.22%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.10%\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Epoch 6: validation accuracy 98.30%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.19%\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Epoch 7: validation accuracy 98.37%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.20%\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Epoch 8: validation accuracy 98.43%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.26%\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 9: validation accuracy 98.46%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.33%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Epoch 10: validation accuracy 98.44%\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Epoch 11: validation accuracy 98.50%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.34%\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Epoch 12: validation accuracy 98.54%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.34%\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Epoch 13: validation accuracy 98.55%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.36%\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 14: validation accuracy 98.54%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Epoch 15: validation accuracy 98.57%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.39%\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Epoch 16: validation accuracy 98.55%\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Epoch 17: validation accuracy 98.57%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.43%\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Epoch 18: validation accuracy 98.60%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.42%\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 19: validation accuracy 98.61%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.43%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Epoch 20: validation accuracy 98.61%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.43%\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Epoch 21: validation accuracy 98.61%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.47%\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Epoch 22: validation accuracy 98.60%\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Epoch 23: validation accuracy 98.60%\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 24: validation accuracy 98.59%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Epoch 25: validation accuracy 98.58%\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Epoch 26: validation accuracy 98.59%\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Epoch 27: validation accuracy 98.58%\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Epoch 28: validation accuracy 98.59%\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 29: validation accuracy 98.58%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Epoch 30: validation accuracy 98.58%\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Epoch 31: validation accuracy 98.57%\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Epoch 32: validation accuracy 98.57%\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Epoch 33: validation accuracy 98.57%\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 34: validation accuracy 98.55%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Epoch 35: validation accuracy 98.53%\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Epoch 36: validation accuracy 98.50%\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Epoch 37: validation accuracy 98.50%\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Epoch 38: validation accuracy 98.53%\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 39: validation accuracy 98.54%\n",
      "Training mini-batch number 200000\n",
      "Training mini-batch number 201000\n",
      "Training mini-batch number 202000\n",
      "Training mini-batch number 203000\n",
      "Training mini-batch number 204000\n",
      "Epoch 40: validation accuracy 98.53%\n",
      "Training mini-batch number 205000\n",
      "Training mini-batch number 206000\n",
      "Training mini-batch number 207000\n",
      "Training mini-batch number 208000\n",
      "Training mini-batch number 209000\n",
      "Epoch 41: validation accuracy 98.53%\n",
      "Training mini-batch number 210000\n",
      "Training mini-batch number 211000\n",
      "Training mini-batch number 212000\n",
      "Training mini-batch number 213000\n",
      "Training mini-batch number 214000\n",
      "Epoch 42: validation accuracy 98.54%\n",
      "Training mini-batch number 215000\n",
      "Training mini-batch number 216000\n",
      "Training mini-batch number 217000\n",
      "Training mini-batch number 218000\n",
      "Training mini-batch number 219000\n",
      "Epoch 43: validation accuracy 98.54%\n",
      "Training mini-batch number 220000\n",
      "Training mini-batch number 221000\n",
      "Training mini-batch number 222000\n",
      "Training mini-batch number 223000\n",
      "Training mini-batch number 224000\n",
      "Epoch 44: validation accuracy 98.55%\n",
      "Training mini-batch number 225000\n",
      "Training mini-batch number 226000\n",
      "Training mini-batch number 227000\n",
      "Training mini-batch number 228000\n",
      "Training mini-batch number 229000\n",
      "Epoch 45: validation accuracy 98.52%\n",
      "Training mini-batch number 230000\n",
      "Training mini-batch number 231000\n",
      "Training mini-batch number 232000\n",
      "Training mini-batch number 233000\n",
      "Training mini-batch number 234000\n",
      "Epoch 46: validation accuracy 98.53%\n",
      "Training mini-batch number 235000\n",
      "Training mini-batch number 236000\n",
      "Training mini-batch number 237000\n",
      "Training mini-batch number 238000\n",
      "Training mini-batch number 239000\n",
      "Epoch 47: validation accuracy 98.51%\n",
      "Training mini-batch number 240000\n",
      "Training mini-batch number 241000\n",
      "Training mini-batch number 242000\n",
      "Training mini-batch number 243000\n",
      "Training mini-batch number 244000\n",
      "Epoch 48: validation accuracy 98.51%\n",
      "Training mini-batch number 245000\n",
      "Training mini-batch number 246000\n",
      "Training mini-batch number 247000\n",
      "Training mini-batch number 248000\n",
      "Training mini-batch number 249000\n",
      "Epoch 49: validation accuracy 98.51%\n",
      "Training mini-batch number 250000\n",
      "Training mini-batch number 251000\n",
      "Training mini-batch number 252000\n",
      "Training mini-batch number 253000\n",
      "Training mini-batch number 254000\n",
      "Epoch 50: validation accuracy 98.52%\n",
      "Training mini-batch number 255000\n",
      "Training mini-batch number 256000\n",
      "Training mini-batch number 257000\n",
      "Training mini-batch number 258000\n",
      "Training mini-batch number 259000\n",
      "Epoch 51: validation accuracy 98.51%\n",
      "Training mini-batch number 260000\n",
      "Training mini-batch number 261000\n",
      "Training mini-batch number 262000\n",
      "Training mini-batch number 263000\n",
      "Training mini-batch number 264000\n",
      "Epoch 52: validation accuracy 98.52%\n",
      "Training mini-batch number 265000\n",
      "Training mini-batch number 266000\n",
      "Training mini-batch number 267000\n",
      "Training mini-batch number 268000\n",
      "Training mini-batch number 269000\n",
      "Epoch 53: validation accuracy 98.52%\n",
      "Training mini-batch number 270000\n",
      "Training mini-batch number 271000\n",
      "Training mini-batch number 272000\n",
      "Training mini-batch number 273000\n",
      "Training mini-batch number 274000\n",
      "Epoch 54: validation accuracy 98.53%\n",
      "Training mini-batch number 275000\n",
      "Training mini-batch number 276000\n",
      "Training mini-batch number 277000\n",
      "Training mini-batch number 278000\n",
      "Training mini-batch number 279000\n",
      "Epoch 55: validation accuracy 98.52%\n",
      "Training mini-batch number 280000\n",
      "Training mini-batch number 281000\n",
      "Training mini-batch number 282000\n",
      "Training mini-batch number 283000\n",
      "Training mini-batch number 284000\n",
      "Epoch 56: validation accuracy 98.53%\n",
      "Training mini-batch number 285000\n",
      "Training mini-batch number 286000\n",
      "Training mini-batch number 287000\n",
      "Training mini-batch number 288000\n",
      "Training mini-batch number 289000\n",
      "Epoch 57: validation accuracy 98.53%\n",
      "Training mini-batch number 290000\n",
      "Training mini-batch number 291000\n",
      "Training mini-batch number 292000\n",
      "Training mini-batch number 293000\n",
      "Training mini-batch number 294000\n",
      "Epoch 58: validation accuracy 98.51%\n",
      "Training mini-batch number 295000\n",
      "Training mini-batch number 296000\n",
      "Training mini-batch number 297000\n",
      "Training mini-batch number 298000\n",
      "Training mini-batch number 299000\n",
      "Epoch 59: validation accuracy 98.51%\n",
      "Finished training network.\n",
      "Best validation accuracy of 98.61% obtained at iteration 109999\n",
      "Corresponding test accuracy of 98.47%\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a second convolutional layer: \n",
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
    "                     filter_shape=(20, 1, 5, 5),\n",
    "                     poolsize=(2, 2)),\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
    "                     filter_shape=(40, 20, 5, 5),\n",
    "                     poolsize=(2, 2)),\n",
    "        FullyConnectedLayer(n_in=40*4*4, n_out=100),\n",
    "        SoftmaxLayer(n_in=100, n_out=10)\n",
    "    ], mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Epoch 0: validation accuracy 92.67%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 92.43%\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Epoch 1: validation accuracy 96.80%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.53%\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Epoch 2: validation accuracy 97.63%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.43%\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Epoch 3: validation accuracy 97.89%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.74%\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 4: validation accuracy 98.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.92%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Epoch 5: validation accuracy 98.15%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.14%\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Epoch 6: validation accuracy 98.21%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.31%\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Epoch 7: validation accuracy 98.27%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.44%\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Epoch 8: validation accuracy 98.32%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.56%\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 9: validation accuracy 98.39%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.58%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Epoch 10: validation accuracy 98.48%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.66%\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Epoch 11: validation accuracy 98.55%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.70%\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Epoch 12: validation accuracy 98.59%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.77%\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Epoch 13: validation accuracy 98.65%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.80%\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 14: validation accuracy 98.65%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.81%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Epoch 15: validation accuracy 98.70%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.86%\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Epoch 16: validation accuracy 98.75%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.87%\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Epoch 17: validation accuracy 98.77%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.88%\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Epoch 18: validation accuracy 98.82%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.86%\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 19: validation accuracy 98.84%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.88%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Epoch 20: validation accuracy 98.85%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.91%\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Epoch 21: validation accuracy 98.84%\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Epoch 22: validation accuracy 98.85%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.93%\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Epoch 23: validation accuracy 98.86%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.95%\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 24: validation accuracy 98.85%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Epoch 25: validation accuracy 98.86%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.98%\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Epoch 26: validation accuracy 98.85%\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Epoch 27: validation accuracy 98.84%\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Epoch 28: validation accuracy 98.84%\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 29: validation accuracy 98.85%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Epoch 30: validation accuracy 98.87%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.04%\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Epoch 31: validation accuracy 98.87%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.03%\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Epoch 32: validation accuracy 98.88%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.03%\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Epoch 33: validation accuracy 98.89%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.05%\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 34: validation accuracy 98.91%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.06%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Epoch 35: validation accuracy 98.92%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.06%\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Epoch 36: validation accuracy 98.93%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.07%\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Epoch 37: validation accuracy 98.94%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.06%\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Epoch 38: validation accuracy 98.94%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.06%\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 39: validation accuracy 98.95%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.05%\n",
      "Training mini-batch number 200000\n",
      "Training mini-batch number 201000\n",
      "Training mini-batch number 202000\n",
      "Training mini-batch number 203000\n",
      "Training mini-batch number 204000\n",
      "Epoch 40: validation accuracy 98.94%\n",
      "Training mini-batch number 205000\n",
      "Training mini-batch number 206000\n",
      "Training mini-batch number 207000\n",
      "Training mini-batch number 208000\n",
      "Training mini-batch number 209000\n",
      "Epoch 41: validation accuracy 98.92%\n",
      "Training mini-batch number 210000\n",
      "Training mini-batch number 211000\n",
      "Training mini-batch number 212000\n",
      "Training mini-batch number 213000\n",
      "Training mini-batch number 214000\n",
      "Epoch 42: validation accuracy 98.93%\n",
      "Training mini-batch number 215000\n",
      "Training mini-batch number 216000\n",
      "Training mini-batch number 217000\n",
      "Training mini-batch number 218000\n",
      "Training mini-batch number 219000\n",
      "Epoch 43: validation accuracy 98.93%\n",
      "Training mini-batch number 220000\n",
      "Training mini-batch number 221000\n",
      "Training mini-batch number 222000\n",
      "Training mini-batch number 223000\n",
      "Training mini-batch number 224000\n",
      "Epoch 44: validation accuracy 98.93%\n",
      "Training mini-batch number 225000\n",
      "Training mini-batch number 226000\n",
      "Training mini-batch number 227000\n",
      "Training mini-batch number 228000\n",
      "Training mini-batch number 229000\n",
      "Epoch 45: validation accuracy 98.95%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.08%\n",
      "Training mini-batch number 230000\n",
      "Training mini-batch number 231000\n",
      "Training mini-batch number 232000\n",
      "Training mini-batch number 233000\n",
      "Training mini-batch number 234000\n",
      "Epoch 46: validation accuracy 98.96%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.08%\n",
      "Training mini-batch number 235000\n",
      "Training mini-batch number 236000\n",
      "Training mini-batch number 237000\n",
      "Training mini-batch number 238000\n",
      "Training mini-batch number 239000\n",
      "Epoch 47: validation accuracy 98.95%\n",
      "Training mini-batch number 240000\n",
      "Training mini-batch number 241000\n",
      "Training mini-batch number 242000\n",
      "Training mini-batch number 243000\n",
      "Training mini-batch number 244000\n",
      "Epoch 48: validation accuracy 98.96%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.08%\n",
      "Training mini-batch number 245000\n",
      "Training mini-batch number 246000\n",
      "Training mini-batch number 247000\n",
      "Training mini-batch number 248000\n",
      "Training mini-batch number 249000\n",
      "Epoch 49: validation accuracy 98.95%\n",
      "Training mini-batch number 250000\n",
      "Training mini-batch number 251000\n",
      "Training mini-batch number 252000\n",
      "Training mini-batch number 253000\n",
      "Training mini-batch number 254000\n",
      "Epoch 50: validation accuracy 98.97%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.07%\n",
      "Training mini-batch number 255000\n",
      "Training mini-batch number 256000\n",
      "Training mini-batch number 257000\n",
      "Training mini-batch number 258000\n",
      "Training mini-batch number 259000\n",
      "Epoch 51: validation accuracy 98.98%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.07%\n",
      "Training mini-batch number 260000\n",
      "Training mini-batch number 261000\n",
      "Training mini-batch number 262000\n",
      "Training mini-batch number 263000\n",
      "Training mini-batch number 264000\n",
      "Epoch 52: validation accuracy 98.97%\n",
      "Training mini-batch number 265000\n",
      "Training mini-batch number 266000\n",
      "Training mini-batch number 267000\n",
      "Training mini-batch number 268000\n",
      "Training mini-batch number 269000\n",
      "Epoch 53: validation accuracy 98.97%\n",
      "Training mini-batch number 270000\n",
      "Training mini-batch number 271000\n",
      "Training mini-batch number 272000\n",
      "Training mini-batch number 273000\n",
      "Training mini-batch number 274000\n",
      "Epoch 54: validation accuracy 98.98%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.08%\n",
      "Training mini-batch number 275000\n",
      "Training mini-batch number 276000\n",
      "Training mini-batch number 277000\n",
      "Training mini-batch number 278000\n",
      "Training mini-batch number 279000\n",
      "Epoch 55: validation accuracy 98.98%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.08%\n",
      "Training mini-batch number 280000\n",
      "Training mini-batch number 281000\n",
      "Training mini-batch number 282000\n",
      "Training mini-batch number 283000\n",
      "Training mini-batch number 284000\n",
      "Epoch 56: validation accuracy 98.98%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.09%\n",
      "Training mini-batch number 285000\n",
      "Training mini-batch number 286000\n",
      "Training mini-batch number 287000\n",
      "Training mini-batch number 288000\n",
      "Training mini-batch number 289000\n",
      "Epoch 57: validation accuracy 98.98%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.10%\n",
      "Training mini-batch number 290000\n",
      "Training mini-batch number 291000\n",
      "Training mini-batch number 292000\n",
      "Training mini-batch number 293000\n",
      "Training mini-batch number 294000\n",
      "Epoch 58: validation accuracy 98.98%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.10%\n",
      "Training mini-batch number 295000\n",
      "Training mini-batch number 296000\n",
      "Training mini-batch number 297000\n",
      "Training mini-batch number 298000\n",
      "Training mini-batch number 299000\n",
      "Epoch 59: validation accuracy 98.98%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.11%\n",
      "Finished training network.\n",
      "Best validation accuracy of 98.98% obtained at iteration 299999\n",
      "Corresponding test accuracy of 99.11%\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch from sigmoid to tanh neurons: \n",
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
    "                     filter_shape=(20, 1, 5, 5),\n",
    "                     poolsize=(2, 2),\n",
    "                     activation_fn=tanh),\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
    "                     filter_shape=(40, 20, 5, 5),\n",
    "                     poolsize=(2, 2),\n",
    "                     activation_fn=tanh),\n",
    "        FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=tanh),\n",
    "        SoftmaxLayer(n_in=100, n_out=10)\n",
    "    ], mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Epoch 0: validation accuracy 97.16%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.19%\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Epoch 1: validation accuracy 98.10%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.03%\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Epoch 2: validation accuracy 98.28%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.35%\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Epoch 3: validation accuracy 98.32%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.14%\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 4: validation accuracy 98.40%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.40%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Epoch 5: validation accuracy 98.68%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.55%\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Epoch 6: validation accuracy 98.79%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.67%\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Epoch 7: validation accuracy 98.77%\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Epoch 8: validation accuracy 98.78%\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 9: validation accuracy 98.92%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.97%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Epoch 10: validation accuracy 98.84%\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Epoch 11: validation accuracy 98.88%\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Epoch 12: validation accuracy 98.89%\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Epoch 13: validation accuracy 98.87%\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 14: validation accuracy 98.84%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Epoch 15: validation accuracy 98.85%\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Epoch 16: validation accuracy 98.85%\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Epoch 17: validation accuracy 98.84%\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Epoch 18: validation accuracy 98.86%\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 19: validation accuracy 98.86%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Epoch 20: validation accuracy 98.85%\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Epoch 21: validation accuracy 98.85%\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Epoch 22: validation accuracy 98.86%\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Epoch 23: validation accuracy 98.86%\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 24: validation accuracy 98.86%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Epoch 25: validation accuracy 98.85%\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Epoch 26: validation accuracy 98.85%\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Epoch 27: validation accuracy 98.85%\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Epoch 28: validation accuracy 98.85%\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 29: validation accuracy 98.85%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Epoch 30: validation accuracy 98.85%\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Epoch 31: validation accuracy 98.87%\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Epoch 32: validation accuracy 98.87%\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Epoch 33: validation accuracy 98.88%\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 34: validation accuracy 98.88%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Epoch 35: validation accuracy 98.88%\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Epoch 36: validation accuracy 98.88%\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Epoch 37: validation accuracy 98.89%\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Epoch 38: validation accuracy 98.89%\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 39: validation accuracy 98.89%\n",
      "Training mini-batch number 200000\n",
      "Training mini-batch number 201000\n",
      "Training mini-batch number 202000\n",
      "Training mini-batch number 203000\n",
      "Training mini-batch number 204000\n",
      "Epoch 40: validation accuracy 98.89%\n",
      "Training mini-batch number 205000\n",
      "Training mini-batch number 206000\n",
      "Training mini-batch number 207000\n",
      "Training mini-batch number 208000\n",
      "Training mini-batch number 209000\n",
      "Epoch 41: validation accuracy 98.89%\n",
      "Training mini-batch number 210000\n",
      "Training mini-batch number 211000\n",
      "Training mini-batch number 212000\n",
      "Training mini-batch number 213000\n",
      "Training mini-batch number 214000\n",
      "Epoch 42: validation accuracy 98.90%\n",
      "Training mini-batch number 215000\n",
      "Training mini-batch number 216000\n",
      "Training mini-batch number 217000\n",
      "Training mini-batch number 218000\n",
      "Training mini-batch number 219000\n",
      "Epoch 43: validation accuracy 98.89%\n",
      "Training mini-batch number 220000\n",
      "Training mini-batch number 221000\n",
      "Training mini-batch number 222000\n",
      "Training mini-batch number 223000\n",
      "Training mini-batch number 224000\n",
      "Epoch 44: validation accuracy 98.89%\n",
      "Training mini-batch number 225000\n",
      "Training mini-batch number 226000\n",
      "Training mini-batch number 227000\n",
      "Training mini-batch number 228000\n",
      "Training mini-batch number 229000\n",
      "Epoch 45: validation accuracy 98.89%\n",
      "Training mini-batch number 230000\n",
      "Training mini-batch number 231000\n",
      "Training mini-batch number 232000\n",
      "Training mini-batch number 233000\n",
      "Training mini-batch number 234000\n",
      "Epoch 46: validation accuracy 98.89%\n",
      "Training mini-batch number 235000\n",
      "Training mini-batch number 236000\n",
      "Training mini-batch number 237000\n",
      "Training mini-batch number 238000\n",
      "Training mini-batch number 239000\n",
      "Epoch 47: validation accuracy 98.89%\n",
      "Training mini-batch number 240000\n",
      "Training mini-batch number 241000\n",
      "Training mini-batch number 242000\n",
      "Training mini-batch number 243000\n",
      "Training mini-batch number 244000\n",
      "Epoch 48: validation accuracy 98.89%\n",
      "Training mini-batch number 245000\n",
      "Training mini-batch number 246000\n",
      "Training mini-batch number 247000\n",
      "Training mini-batch number 248000\n",
      "Training mini-batch number 249000\n",
      "Epoch 49: validation accuracy 98.88%\n",
      "Training mini-batch number 250000\n",
      "Training mini-batch number 251000\n",
      "Training mini-batch number 252000\n",
      "Training mini-batch number 253000\n",
      "Training mini-batch number 254000\n",
      "Epoch 50: validation accuracy 98.88%\n",
      "Training mini-batch number 255000\n",
      "Training mini-batch number 256000\n",
      "Training mini-batch number 257000\n",
      "Training mini-batch number 258000\n",
      "Training mini-batch number 259000\n",
      "Epoch 51: validation accuracy 98.88%\n",
      "Training mini-batch number 260000\n",
      "Training mini-batch number 261000\n",
      "Training mini-batch number 262000\n",
      "Training mini-batch number 263000\n",
      "Training mini-batch number 264000\n",
      "Epoch 52: validation accuracy 98.88%\n",
      "Training mini-batch number 265000\n",
      "Training mini-batch number 266000\n",
      "Training mini-batch number 267000\n",
      "Training mini-batch number 268000\n",
      "Training mini-batch number 269000\n",
      "Epoch 53: validation accuracy 98.88%\n",
      "Training mini-batch number 270000\n",
      "Training mini-batch number 271000\n",
      "Training mini-batch number 272000\n",
      "Training mini-batch number 273000\n",
      "Training mini-batch number 274000\n",
      "Epoch 54: validation accuracy 98.88%\n",
      "Training mini-batch number 275000\n",
      "Training mini-batch number 276000\n",
      "Training mini-batch number 277000\n",
      "Training mini-batch number 278000\n",
      "Training mini-batch number 279000\n",
      "Epoch 55: validation accuracy 98.88%\n",
      "Training mini-batch number 280000\n",
      "Training mini-batch number 281000\n",
      "Training mini-batch number 282000\n",
      "Training mini-batch number 283000\n",
      "Training mini-batch number 284000\n",
      "Epoch 56: validation accuracy 98.89%\n",
      "Training mini-batch number 285000\n",
      "Training mini-batch number 286000\n",
      "Training mini-batch number 287000\n",
      "Training mini-batch number 288000\n",
      "Training mini-batch number 289000\n",
      "Epoch 57: validation accuracy 98.89%\n",
      "Training mini-batch number 290000\n",
      "Training mini-batch number 291000\n",
      "Training mini-batch number 292000\n",
      "Training mini-batch number 293000\n",
      "Training mini-batch number 294000\n",
      "Epoch 58: validation accuracy 98.90%\n",
      "Training mini-batch number 295000\n",
      "Training mini-batch number 296000\n",
      "Training mini-batch number 297000\n",
      "Training mini-batch number 298000\n",
      "Training mini-batch number 299000\n",
      "Epoch 59: validation accuracy 98.90%\n",
      "Finished training network.\n",
      "Best validation accuracy of 98.92% obtained at iteration 49999\n",
      "Corresponding test accuracy of 98.97%\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch to ReLU neurons: \n",
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
    "                      filter_shape=(20, 1, 5, 5), \n",
    "                      poolsize=(2, 2), \n",
    "                      activation_fn=ReLU),\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12), \n",
    "                      filter_shape=(40, 20, 5, 5), \n",
    "                      poolsize=(2, 2), \n",
    "                      activation_fn=ReLU),\n",
    "        FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
    "        SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Epoch 0: validation accuracy 97.54%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.26%\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Epoch 1: validation accuracy 98.05%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.00%\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Epoch 2: validation accuracy 98.29%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.27%\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Epoch 3: validation accuracy 98.57%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.57%\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 4: validation accuracy 98.68%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.61%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Epoch 5: validation accuracy 98.82%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.77%\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Epoch 6: validation accuracy 98.75%\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Epoch 7: validation accuracy 98.71%\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Epoch 8: validation accuracy 98.77%\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 9: validation accuracy 98.90%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.76%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Epoch 10: validation accuracy 98.74%\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Epoch 11: validation accuracy 98.74%\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Epoch 12: validation accuracy 98.94%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.93%\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Epoch 13: validation accuracy 98.93%\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 14: validation accuracy 98.75%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Epoch 15: validation accuracy 98.77%\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Epoch 16: validation accuracy 98.95%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.98%\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Epoch 17: validation accuracy 98.91%\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Epoch 18: validation accuracy 98.66%\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 19: validation accuracy 98.80%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Epoch 20: validation accuracy 98.82%\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Epoch 21: validation accuracy 98.84%\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Epoch 22: validation accuracy 98.81%\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Epoch 23: validation accuracy 99.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.00%\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 24: validation accuracy 98.87%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Epoch 25: validation accuracy 99.04%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.99%\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Epoch 26: validation accuracy 98.79%\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Epoch 27: validation accuracy 99.02%\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Epoch 28: validation accuracy 99.11%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.09%\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 29: validation accuracy 99.20%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.09%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Epoch 30: validation accuracy 99.20%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.11%\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Epoch 31: validation accuracy 99.19%\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Epoch 32: validation accuracy 99.19%\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Epoch 33: validation accuracy 99.19%\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 34: validation accuracy 99.17%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Epoch 35: validation accuracy 99.18%\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Epoch 36: validation accuracy 99.18%\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Epoch 37: validation accuracy 99.18%\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Epoch 38: validation accuracy 99.17%\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 39: validation accuracy 99.17%\n",
      "Training mini-batch number 200000\n",
      "Training mini-batch number 201000\n",
      "Training mini-batch number 202000\n",
      "Training mini-batch number 203000\n",
      "Training mini-batch number 204000\n",
      "Epoch 40: validation accuracy 99.18%\n",
      "Training mini-batch number 205000\n",
      "Training mini-batch number 206000\n",
      "Training mini-batch number 207000\n",
      "Training mini-batch number 208000\n",
      "Training mini-batch number 209000\n",
      "Epoch 41: validation accuracy 99.17%\n",
      "Training mini-batch number 210000\n",
      "Training mini-batch number 211000\n",
      "Training mini-batch number 212000\n",
      "Training mini-batch number 213000\n",
      "Training mini-batch number 214000\n",
      "Epoch 42: validation accuracy 99.18%\n",
      "Training mini-batch number 215000\n",
      "Training mini-batch number 216000\n",
      "Training mini-batch number 217000\n",
      "Training mini-batch number 218000\n",
      "Training mini-batch number 219000\n",
      "Epoch 43: validation accuracy 99.16%\n",
      "Training mini-batch number 220000\n",
      "Training mini-batch number 221000\n",
      "Training mini-batch number 222000\n",
      "Training mini-batch number 223000\n",
      "Training mini-batch number 224000\n",
      "Epoch 44: validation accuracy 99.17%\n",
      "Training mini-batch number 225000\n",
      "Training mini-batch number 226000\n",
      "Training mini-batch number 227000\n",
      "Training mini-batch number 228000\n",
      "Training mini-batch number 229000\n",
      "Epoch 45: validation accuracy 99.15%\n",
      "Training mini-batch number 230000\n",
      "Training mini-batch number 231000\n",
      "Training mini-batch number 232000\n",
      "Training mini-batch number 233000\n",
      "Training mini-batch number 234000\n",
      "Epoch 46: validation accuracy 99.15%\n",
      "Training mini-batch number 235000\n",
      "Training mini-batch number 236000\n",
      "Training mini-batch number 237000\n",
      "Training mini-batch number 238000\n",
      "Training mini-batch number 239000\n",
      "Epoch 47: validation accuracy 99.15%\n",
      "Training mini-batch number 240000\n",
      "Training mini-batch number 241000\n",
      "Training mini-batch number 242000\n",
      "Training mini-batch number 243000\n",
      "Training mini-batch number 244000\n",
      "Epoch 48: validation accuracy 99.16%\n",
      "Training mini-batch number 245000\n",
      "Training mini-batch number 246000\n",
      "Training mini-batch number 247000\n",
      "Training mini-batch number 248000\n",
      "Training mini-batch number 249000\n",
      "Epoch 49: validation accuracy 99.17%\n",
      "Training mini-batch number 250000\n",
      "Training mini-batch number 251000\n",
      "Training mini-batch number 252000\n",
      "Training mini-batch number 253000\n",
      "Training mini-batch number 254000\n",
      "Epoch 50: validation accuracy 99.17%\n",
      "Training mini-batch number 255000\n",
      "Training mini-batch number 256000\n",
      "Training mini-batch number 257000\n",
      "Training mini-batch number 258000\n",
      "Training mini-batch number 259000\n",
      "Epoch 51: validation accuracy 99.17%\n",
      "Training mini-batch number 260000\n",
      "Training mini-batch number 261000\n",
      "Training mini-batch number 262000\n",
      "Training mini-batch number 263000\n",
      "Training mini-batch number 264000\n",
      "Epoch 52: validation accuracy 99.17%\n",
      "Training mini-batch number 265000\n",
      "Training mini-batch number 266000\n",
      "Training mini-batch number 267000\n",
      "Training mini-batch number 268000\n",
      "Training mini-batch number 269000\n",
      "Epoch 53: validation accuracy 99.17%\n",
      "Training mini-batch number 270000\n",
      "Training mini-batch number 271000\n",
      "Training mini-batch number 272000\n",
      "Training mini-batch number 273000\n",
      "Training mini-batch number 274000\n",
      "Epoch 54: validation accuracy 99.18%\n",
      "Training mini-batch number 275000\n",
      "Training mini-batch number 276000\n",
      "Training mini-batch number 277000\n",
      "Training mini-batch number 278000\n",
      "Training mini-batch number 279000\n",
      "Epoch 55: validation accuracy 99.18%\n",
      "Training mini-batch number 280000\n",
      "Training mini-batch number 281000\n",
      "Training mini-batch number 282000\n",
      "Training mini-batch number 283000\n",
      "Training mini-batch number 284000\n",
      "Epoch 56: validation accuracy 99.18%\n",
      "Training mini-batch number 285000\n",
      "Training mini-batch number 286000\n",
      "Training mini-batch number 287000\n",
      "Training mini-batch number 288000\n",
      "Training mini-batch number 289000\n",
      "Epoch 57: validation accuracy 99.18%\n",
      "Training mini-batch number 290000\n",
      "Training mini-batch number 291000\n",
      "Training mini-batch number 292000\n",
      "Training mini-batch number 293000\n",
      "Training mini-batch number 294000\n",
      "Epoch 58: validation accuracy 99.18%\n",
      "Training mini-batch number 295000\n",
      "Training mini-batch number 296000\n",
      "Training mini-batch number 297000\n",
      "Training mini-batch number 298000\n",
      "Training mini-batch number 299000\n",
      "Epoch 59: validation accuracy 99.18%\n",
      "Finished training network.\n",
      "Best validation accuracy of 99.20% obtained at iteration 154999\n",
      "Corresponding test accuracy of 99.11%\n"
     ]
    }
   ],
   "source": [
    "# note the eta is an order of magnitude smaller, and lambda has been added as a parameter: \n",
    "net.SGD(training_data, 60, mini_batch_size, 0.03, \n",
    "            validation_data, test_data, lmbda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expanded_training_data, _, _ = load_data_shared(\"../../neural-networks-and-deep-learning/data/mnist_expanded.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
    "                      filter_shape=(20, 1, 5, 5), \n",
    "                      poolsize=(2, 2), \n",
    "                      activation_fn=ReLU),\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12), \n",
    "                      filter_shape=(40, 20, 5, 5), \n",
    "                      poolsize=(2, 2), \n",
    "                      activation_fn=ReLU),\n",
    "        FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
    "        SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 0: validation accuracy 96.92%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.05%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 1: validation accuracy 98.79%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.87%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 2: validation accuracy 98.99%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.88%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 3: validation accuracy 99.11%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.93%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 4: validation accuracy 99.07%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 5: validation accuracy 99.12%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.98%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 6: validation accuracy 99.16%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.94%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 7: validation accuracy 99.16%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.06%\n",
      "Training mini-batch number 200000\n",
      "Training mini-batch number 201000\n",
      "Training mini-batch number 202000\n",
      "Training mini-batch number 203000\n",
      "Training mini-batch number 204000\n",
      "Training mini-batch number 205000\n",
      "Training mini-batch number 206000\n",
      "Training mini-batch number 207000\n",
      "Training mini-batch number 208000\n",
      "Training mini-batch number 209000\n",
      "Training mini-batch number 210000\n",
      "Training mini-batch number 211000\n",
      "Training mini-batch number 212000\n",
      "Training mini-batch number 213000\n",
      "Training mini-batch number 214000\n",
      "Training mini-batch number 215000\n",
      "Training mini-batch number 216000\n",
      "Training mini-batch number 217000\n",
      "Training mini-batch number 218000\n",
      "Training mini-batch number 219000\n",
      "Training mini-batch number 220000\n",
      "Training mini-batch number 221000\n",
      "Training mini-batch number 222000\n",
      "Training mini-batch number 223000\n",
      "Training mini-batch number 224000\n",
      "Epoch 8: validation accuracy 99.27%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.19%\n",
      "Training mini-batch number 225000\n",
      "Training mini-batch number 226000\n",
      "Training mini-batch number 227000\n",
      "Training mini-batch number 228000\n",
      "Training mini-batch number 229000\n",
      "Training mini-batch number 230000\n",
      "Training mini-batch number 231000\n",
      "Training mini-batch number 232000\n",
      "Training mini-batch number 233000\n",
      "Training mini-batch number 234000\n",
      "Training mini-batch number 235000\n",
      "Training mini-batch number 236000\n",
      "Training mini-batch number 237000\n",
      "Training mini-batch number 238000\n",
      "Training mini-batch number 239000\n",
      "Training mini-batch number 240000\n",
      "Training mini-batch number 241000\n",
      "Training mini-batch number 242000\n",
      "Training mini-batch number 243000\n",
      "Training mini-batch number 244000\n",
      "Training mini-batch number 245000\n",
      "Training mini-batch number 246000\n",
      "Training mini-batch number 247000\n",
      "Training mini-batch number 248000\n",
      "Training mini-batch number 249000\n",
      "Epoch 9: validation accuracy 99.31%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.17%\n",
      "Training mini-batch number 250000\n",
      "Training mini-batch number 251000\n",
      "Training mini-batch number 252000\n",
      "Training mini-batch number 253000\n",
      "Training mini-batch number 254000\n",
      "Training mini-batch number 255000\n",
      "Training mini-batch number 256000\n",
      "Training mini-batch number 257000\n",
      "Training mini-batch number 258000\n",
      "Training mini-batch number 259000\n",
      "Training mini-batch number 260000\n",
      "Training mini-batch number 261000\n",
      "Training mini-batch number 262000\n",
      "Training mini-batch number 263000\n",
      "Training mini-batch number 264000\n",
      "Training mini-batch number 265000\n",
      "Training mini-batch number 266000\n",
      "Training mini-batch number 267000\n",
      "Training mini-batch number 268000\n",
      "Training mini-batch number 269000\n",
      "Training mini-batch number 270000\n",
      "Training mini-batch number 271000\n",
      "Training mini-batch number 272000\n",
      "Training mini-batch number 273000\n",
      "Training mini-batch number 274000\n",
      "Epoch 10: validation accuracy 99.26%\n",
      "Training mini-batch number 275000\n",
      "Training mini-batch number 276000\n",
      "Training mini-batch number 277000\n",
      "Training mini-batch number 278000\n",
      "Training mini-batch number 279000\n",
      "Training mini-batch number 280000\n",
      "Training mini-batch number 281000\n",
      "Training mini-batch number 282000\n",
      "Training mini-batch number 283000\n",
      "Training mini-batch number 284000\n",
      "Training mini-batch number 285000\n",
      "Training mini-batch number 286000\n",
      "Training mini-batch number 287000\n",
      "Training mini-batch number 288000\n",
      "Training mini-batch number 289000\n",
      "Training mini-batch number 290000\n",
      "Training mini-batch number 291000\n",
      "Training mini-batch number 292000\n",
      "Training mini-batch number 293000\n",
      "Training mini-batch number 294000\n",
      "Training mini-batch number 295000\n",
      "Training mini-batch number 296000\n",
      "Training mini-batch number 297000\n",
      "Training mini-batch number 298000\n",
      "Training mini-batch number 299000\n",
      "Epoch 11: validation accuracy 99.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.24%\n",
      "Training mini-batch number 300000\n",
      "Training mini-batch number 301000\n",
      "Training mini-batch number 302000\n",
      "Training mini-batch number 303000\n",
      "Training mini-batch number 304000\n",
      "Training mini-batch number 305000\n",
      "Training mini-batch number 306000\n",
      "Training mini-batch number 307000\n",
      "Training mini-batch number 308000\n",
      "Training mini-batch number 309000\n",
      "Training mini-batch number 310000\n",
      "Training mini-batch number 311000\n",
      "Training mini-batch number 312000\n",
      "Training mini-batch number 313000\n",
      "Training mini-batch number 314000\n",
      "Training mini-batch number 315000\n",
      "Training mini-batch number 316000\n",
      "Training mini-batch number 317000\n",
      "Training mini-batch number 318000\n",
      "Training mini-batch number 319000\n",
      "Training mini-batch number 320000\n",
      "Training mini-batch number 321000\n",
      "Training mini-batch number 322000\n",
      "Training mini-batch number 323000\n",
      "Training mini-batch number 324000\n",
      "Epoch 12: validation accuracy 99.17%\n",
      "Training mini-batch number 325000\n",
      "Training mini-batch number 326000\n",
      "Training mini-batch number 327000\n",
      "Training mini-batch number 328000\n",
      "Training mini-batch number 329000\n",
      "Training mini-batch number 330000\n",
      "Training mini-batch number 331000\n",
      "Training mini-batch number 332000\n",
      "Training mini-batch number 333000\n",
      "Training mini-batch number 334000\n",
      "Training mini-batch number 335000\n",
      "Training mini-batch number 336000\n",
      "Training mini-batch number 337000\n",
      "Training mini-batch number 338000\n",
      "Training mini-batch number 339000\n",
      "Training mini-batch number 340000\n",
      "Training mini-batch number 341000\n",
      "Training mini-batch number 342000\n",
      "Training mini-batch number 343000\n",
      "Training mini-batch number 344000\n",
      "Training mini-batch number 345000\n",
      "Training mini-batch number 346000\n",
      "Training mini-batch number 347000\n",
      "Training mini-batch number 348000\n",
      "Training mini-batch number 349000\n",
      "Epoch 13: validation accuracy 99.23%\n",
      "Training mini-batch number 350000\n",
      "Training mini-batch number 351000\n",
      "Training mini-batch number 352000\n",
      "Training mini-batch number 353000\n",
      "Training mini-batch number 354000\n",
      "Training mini-batch number 355000\n",
      "Training mini-batch number 356000\n",
      "Training mini-batch number 357000\n",
      "Training mini-batch number 358000\n",
      "Training mini-batch number 359000\n",
      "Training mini-batch number 360000\n",
      "Training mini-batch number 361000\n",
      "Training mini-batch number 362000\n",
      "Training mini-batch number 363000\n",
      "Training mini-batch number 364000\n",
      "Training mini-batch number 365000\n",
      "Training mini-batch number 366000\n",
      "Training mini-batch number 367000\n",
      "Training mini-batch number 368000\n",
      "Training mini-batch number 369000\n",
      "Training mini-batch number 370000\n",
      "Training mini-batch number 371000\n",
      "Training mini-batch number 372000\n",
      "Training mini-batch number 373000\n",
      "Training mini-batch number 374000\n",
      "Epoch 14: validation accuracy 99.15%\n",
      "Training mini-batch number 375000\n",
      "Training mini-batch number 376000\n",
      "Training mini-batch number 377000\n",
      "Training mini-batch number 378000\n",
      "Training mini-batch number 379000\n",
      "Training mini-batch number 380000\n",
      "Training mini-batch number 381000\n",
      "Training mini-batch number 382000\n",
      "Training mini-batch number 383000\n",
      "Training mini-batch number 384000\n",
      "Training mini-batch number 385000\n",
      "Training mini-batch number 386000\n",
      "Training mini-batch number 387000\n",
      "Training mini-batch number 388000\n",
      "Training mini-batch number 389000\n",
      "Training mini-batch number 390000\n",
      "Training mini-batch number 391000\n",
      "Training mini-batch number 392000\n",
      "Training mini-batch number 393000\n",
      "Training mini-batch number 394000\n",
      "Training mini-batch number 395000\n",
      "Training mini-batch number 396000\n",
      "Training mini-batch number 397000\n",
      "Training mini-batch number 398000\n",
      "Training mini-batch number 399000\n",
      "Epoch 15: validation accuracy 99.26%\n",
      "Training mini-batch number 400000\n",
      "Training mini-batch number 401000\n",
      "Training mini-batch number 402000\n",
      "Training mini-batch number 403000\n",
      "Training mini-batch number 404000\n",
      "Training mini-batch number 405000\n",
      "Training mini-batch number 406000\n",
      "Training mini-batch number 407000\n",
      "Training mini-batch number 408000\n",
      "Training mini-batch number 409000\n",
      "Training mini-batch number 410000\n",
      "Training mini-batch number 411000\n",
      "Training mini-batch number 412000\n",
      "Training mini-batch number 413000\n",
      "Training mini-batch number 414000\n",
      "Training mini-batch number 415000\n",
      "Training mini-batch number 416000\n",
      "Training mini-batch number 417000\n",
      "Training mini-batch number 418000\n",
      "Training mini-batch number 419000\n",
      "Training mini-batch number 420000\n",
      "Training mini-batch number 421000\n",
      "Training mini-batch number 422000\n",
      "Training mini-batch number 423000\n",
      "Training mini-batch number 424000\n",
      "Epoch 16: validation accuracy 99.35%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.28%\n",
      "Training mini-batch number 425000\n",
      "Training mini-batch number 426000\n",
      "Training mini-batch number 427000\n",
      "Training mini-batch number 428000\n",
      "Training mini-batch number 429000\n",
      "Training mini-batch number 430000\n",
      "Training mini-batch number 431000\n",
      "Training mini-batch number 432000\n",
      "Training mini-batch number 433000\n",
      "Training mini-batch number 434000\n",
      "Training mini-batch number 435000\n",
      "Training mini-batch number 436000\n",
      "Training mini-batch number 437000\n",
      "Training mini-batch number 438000\n",
      "Training mini-batch number 439000\n",
      "Training mini-batch number 440000\n",
      "Training mini-batch number 441000\n",
      "Training mini-batch number 442000\n",
      "Training mini-batch number 443000\n",
      "Training mini-batch number 444000\n",
      "Training mini-batch number 445000\n",
      "Training mini-batch number 446000\n",
      "Training mini-batch number 447000\n",
      "Training mini-batch number 448000\n",
      "Training mini-batch number 449000\n",
      "Epoch 17: validation accuracy 99.30%\n",
      "Training mini-batch number 450000\n",
      "Training mini-batch number 451000\n",
      "Training mini-batch number 452000\n",
      "Training mini-batch number 453000\n",
      "Training mini-batch number 454000\n",
      "Training mini-batch number 455000\n",
      "Training mini-batch number 456000\n",
      "Training mini-batch number 457000\n",
      "Training mini-batch number 458000\n",
      "Training mini-batch number 459000\n",
      "Training mini-batch number 460000\n",
      "Training mini-batch number 461000\n",
      "Training mini-batch number 462000\n",
      "Training mini-batch number 463000\n",
      "Training mini-batch number 464000\n",
      "Training mini-batch number 465000\n",
      "Training mini-batch number 466000\n",
      "Training mini-batch number 467000\n",
      "Training mini-batch number 468000\n",
      "Training mini-batch number 469000\n",
      "Training mini-batch number 470000\n",
      "Training mini-batch number 471000\n",
      "Training mini-batch number 472000\n",
      "Training mini-batch number 473000\n",
      "Training mini-batch number 474000\n",
      "Epoch 18: validation accuracy 99.31%\n",
      "Training mini-batch number 475000\n",
      "Training mini-batch number 476000\n",
      "Training mini-batch number 477000\n",
      "Training mini-batch number 478000\n",
      "Training mini-batch number 479000\n",
      "Training mini-batch number 480000\n",
      "Training mini-batch number 481000\n",
      "Training mini-batch number 482000\n",
      "Training mini-batch number 483000\n",
      "Training mini-batch number 484000\n",
      "Training mini-batch number 485000\n",
      "Training mini-batch number 486000\n",
      "Training mini-batch number 487000\n",
      "Training mini-batch number 488000\n",
      "Training mini-batch number 489000\n",
      "Training mini-batch number 490000\n",
      "Training mini-batch number 491000\n",
      "Training mini-batch number 492000\n",
      "Training mini-batch number 493000\n",
      "Training mini-batch number 494000\n",
      "Training mini-batch number 495000\n",
      "Training mini-batch number 496000\n",
      "Training mini-batch number 497000\n",
      "Training mini-batch number 498000\n",
      "Training mini-batch number 499000\n",
      "Epoch 19: validation accuracy 99.40%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.33%\n",
      "Training mini-batch number 500000\n",
      "Training mini-batch number 501000\n",
      "Training mini-batch number 502000\n",
      "Training mini-batch number 503000\n",
      "Training mini-batch number 504000\n",
      "Training mini-batch number 505000\n",
      "Training mini-batch number 506000\n",
      "Training mini-batch number 507000\n",
      "Training mini-batch number 508000\n",
      "Training mini-batch number 509000\n",
      "Training mini-batch number 510000\n",
      "Training mini-batch number 511000\n",
      "Training mini-batch number 512000\n",
      "Training mini-batch number 513000\n",
      "Training mini-batch number 514000\n",
      "Training mini-batch number 515000\n",
      "Training mini-batch number 516000\n",
      "Training mini-batch number 517000\n",
      "Training mini-batch number 518000\n",
      "Training mini-batch number 519000\n",
      "Training mini-batch number 520000\n",
      "Training mini-batch number 521000\n",
      "Training mini-batch number 522000\n",
      "Training mini-batch number 523000\n",
      "Training mini-batch number 524000\n",
      "Epoch 20: validation accuracy 99.33%\n",
      "Training mini-batch number 525000\n",
      "Training mini-batch number 526000\n",
      "Training mini-batch number 527000\n",
      "Training mini-batch number 528000\n",
      "Training mini-batch number 529000\n",
      "Training mini-batch number 530000\n",
      "Training mini-batch number 531000\n",
      "Training mini-batch number 532000\n",
      "Training mini-batch number 533000\n",
      "Training mini-batch number 534000\n",
      "Training mini-batch number 535000\n",
      "Training mini-batch number 536000\n",
      "Training mini-batch number 537000\n",
      "Training mini-batch number 538000\n",
      "Training mini-batch number 539000\n",
      "Training mini-batch number 540000\n",
      "Training mini-batch number 541000\n",
      "Training mini-batch number 542000\n",
      "Training mini-batch number 543000\n",
      "Training mini-batch number 544000\n",
      "Training mini-batch number 545000\n",
      "Training mini-batch number 546000\n",
      "Training mini-batch number 547000\n",
      "Training mini-batch number 548000\n",
      "Training mini-batch number 549000\n",
      "Epoch 21: validation accuracy 99.27%\n",
      "Training mini-batch number 550000\n",
      "Training mini-batch number 551000\n",
      "Training mini-batch number 552000\n",
      "Training mini-batch number 553000\n",
      "Training mini-batch number 554000\n",
      "Training mini-batch number 555000\n",
      "Training mini-batch number 556000\n",
      "Training mini-batch number 557000\n",
      "Training mini-batch number 558000\n",
      "Training mini-batch number 559000\n",
      "Training mini-batch number 560000\n",
      "Training mini-batch number 561000\n",
      "Training mini-batch number 562000\n",
      "Training mini-batch number 563000\n",
      "Training mini-batch number 564000\n",
      "Training mini-batch number 565000\n",
      "Training mini-batch number 566000\n",
      "Training mini-batch number 567000\n",
      "Training mini-batch number 568000\n",
      "Training mini-batch number 569000\n",
      "Training mini-batch number 570000\n",
      "Training mini-batch number 571000\n",
      "Training mini-batch number 572000\n",
      "Training mini-batch number 573000\n",
      "Training mini-batch number 574000\n",
      "Epoch 22: validation accuracy 99.40%\n",
      "Training mini-batch number 575000\n",
      "Training mini-batch number 576000\n",
      "Training mini-batch number 577000\n",
      "Training mini-batch number 578000\n",
      "Training mini-batch number 579000\n",
      "Training mini-batch number 580000\n",
      "Training mini-batch number 581000\n",
      "Training mini-batch number 582000\n",
      "Training mini-batch number 583000\n",
      "Training mini-batch number 584000\n",
      "Training mini-batch number 585000\n",
      "Training mini-batch number 586000\n",
      "Training mini-batch number 587000\n",
      "Training mini-batch number 588000\n",
      "Training mini-batch number 589000\n",
      "Training mini-batch number 590000\n",
      "Training mini-batch number 591000\n",
      "Training mini-batch number 592000\n",
      "Training mini-batch number 593000\n",
      "Training mini-batch number 594000\n",
      "Training mini-batch number 595000\n",
      "Training mini-batch number 596000\n",
      "Training mini-batch number 597000\n",
      "Training mini-batch number 598000\n",
      "Training mini-batch number 599000\n",
      "Epoch 23: validation accuracy 99.37%\n",
      "Training mini-batch number 600000\n",
      "Training mini-batch number 601000\n",
      "Training mini-batch number 602000\n",
      "Training mini-batch number 603000\n",
      "Training mini-batch number 604000\n",
      "Training mini-batch number 605000\n",
      "Training mini-batch number 606000\n",
      "Training mini-batch number 607000\n",
      "Training mini-batch number 608000\n",
      "Training mini-batch number 609000\n",
      "Training mini-batch number 610000\n",
      "Training mini-batch number 611000\n",
      "Training mini-batch number 612000\n",
      "Training mini-batch number 613000\n",
      "Training mini-batch number 614000\n",
      "Training mini-batch number 615000\n",
      "Training mini-batch number 616000\n",
      "Training mini-batch number 617000\n",
      "Training mini-batch number 618000\n",
      "Training mini-batch number 619000\n",
      "Training mini-batch number 620000\n",
      "Training mini-batch number 621000\n",
      "Training mini-batch number 622000\n",
      "Training mini-batch number 623000\n",
      "Training mini-batch number 624000\n",
      "Epoch 24: validation accuracy 99.38%\n",
      "Training mini-batch number 625000\n",
      "Training mini-batch number 626000\n",
      "Training mini-batch number 627000\n",
      "Training mini-batch number 628000\n",
      "Training mini-batch number 629000\n",
      "Training mini-batch number 630000\n",
      "Training mini-batch number 631000\n",
      "Training mini-batch number 632000\n",
      "Training mini-batch number 633000\n",
      "Training mini-batch number 634000\n",
      "Training mini-batch number 635000\n",
      "Training mini-batch number 636000\n",
      "Training mini-batch number 637000\n",
      "Training mini-batch number 638000\n",
      "Training mini-batch number 639000\n",
      "Training mini-batch number 640000\n",
      "Training mini-batch number 641000\n",
      "Training mini-batch number 642000\n",
      "Training mini-batch number 643000\n",
      "Training mini-batch number 644000\n",
      "Training mini-batch number 645000\n",
      "Training mini-batch number 646000\n",
      "Training mini-batch number 647000\n",
      "Training mini-batch number 648000\n",
      "Training mini-batch number 649000\n",
      "Epoch 25: validation accuracy 99.27%\n",
      "Training mini-batch number 650000\n",
      "Training mini-batch number 651000\n",
      "Training mini-batch number 652000\n",
      "Training mini-batch number 653000\n",
      "Training mini-batch number 654000\n",
      "Training mini-batch number 655000\n",
      "Training mini-batch number 656000\n",
      "Training mini-batch number 657000\n",
      "Training mini-batch number 658000\n",
      "Training mini-batch number 659000\n",
      "Training mini-batch number 660000\n",
      "Training mini-batch number 661000\n",
      "Training mini-batch number 662000\n",
      "Training mini-batch number 663000\n",
      "Training mini-batch number 664000\n",
      "Training mini-batch number 665000\n",
      "Training mini-batch number 666000\n",
      "Training mini-batch number 667000\n",
      "Training mini-batch number 668000\n",
      "Training mini-batch number 669000\n",
      "Training mini-batch number 670000\n",
      "Training mini-batch number 671000\n",
      "Training mini-batch number 672000\n",
      "Training mini-batch number 673000\n",
      "Training mini-batch number 674000\n",
      "Epoch 26: validation accuracy 99.25%\n",
      "Training mini-batch number 675000\n",
      "Training mini-batch number 676000\n",
      "Training mini-batch number 677000\n",
      "Training mini-batch number 678000\n",
      "Training mini-batch number 679000\n",
      "Training mini-batch number 680000\n",
      "Training mini-batch number 681000\n",
      "Training mini-batch number 682000\n",
      "Training mini-batch number 683000\n",
      "Training mini-batch number 684000\n",
      "Training mini-batch number 685000\n",
      "Training mini-batch number 686000\n",
      "Training mini-batch number 687000\n",
      "Training mini-batch number 688000\n",
      "Training mini-batch number 689000\n",
      "Training mini-batch number 690000\n",
      "Training mini-batch number 691000\n",
      "Training mini-batch number 692000\n",
      "Training mini-batch number 693000\n",
      "Training mini-batch number 694000\n",
      "Training mini-batch number 695000\n",
      "Training mini-batch number 696000\n",
      "Training mini-batch number 697000\n",
      "Training mini-batch number 698000\n",
      "Training mini-batch number 699000\n",
      "Epoch 27: validation accuracy 99.30%\n",
      "Training mini-batch number 700000\n",
      "Training mini-batch number 701000\n",
      "Training mini-batch number 702000\n",
      "Training mini-batch number 703000\n",
      "Training mini-batch number 704000\n",
      "Training mini-batch number 705000\n",
      "Training mini-batch number 706000\n",
      "Training mini-batch number 707000\n",
      "Training mini-batch number 708000\n",
      "Training mini-batch number 709000\n",
      "Training mini-batch number 710000\n",
      "Training mini-batch number 711000\n",
      "Training mini-batch number 712000\n",
      "Training mini-batch number 713000\n",
      "Training mini-batch number 714000\n",
      "Training mini-batch number 715000\n",
      "Training mini-batch number 716000\n",
      "Training mini-batch number 717000\n",
      "Training mini-batch number 718000\n",
      "Training mini-batch number 719000\n",
      "Training mini-batch number 720000\n",
      "Training mini-batch number 721000\n",
      "Training mini-batch number 722000\n",
      "Training mini-batch number 723000\n",
      "Training mini-batch number 724000\n",
      "Epoch 28: validation accuracy 99.30%\n",
      "Training mini-batch number 725000\n",
      "Training mini-batch number 726000\n",
      "Training mini-batch number 727000\n",
      "Training mini-batch number 728000\n",
      "Training mini-batch number 729000\n",
      "Training mini-batch number 730000\n",
      "Training mini-batch number 731000\n",
      "Training mini-batch number 732000\n",
      "Training mini-batch number 733000\n",
      "Training mini-batch number 734000\n",
      "Training mini-batch number 735000\n",
      "Training mini-batch number 736000\n",
      "Training mini-batch number 737000\n",
      "Training mini-batch number 738000\n",
      "Training mini-batch number 739000\n",
      "Training mini-batch number 740000\n",
      "Training mini-batch number 741000\n",
      "Training mini-batch number 742000\n",
      "Training mini-batch number 743000\n",
      "Training mini-batch number 744000\n",
      "Training mini-batch number 745000\n",
      "Training mini-batch number 746000\n",
      "Training mini-batch number 747000\n",
      "Training mini-batch number 748000\n",
      "Training mini-batch number 749000\n",
      "Epoch 29: validation accuracy 99.33%\n",
      "Training mini-batch number 750000\n",
      "Training mini-batch number 751000\n",
      "Training mini-batch number 752000\n",
      "Training mini-batch number 753000\n",
      "Training mini-batch number 754000\n",
      "Training mini-batch number 755000\n",
      "Training mini-batch number 756000\n",
      "Training mini-batch number 757000\n",
      "Training mini-batch number 758000\n",
      "Training mini-batch number 759000\n",
      "Training mini-batch number 760000\n",
      "Training mini-batch number 761000\n",
      "Training mini-batch number 762000\n",
      "Training mini-batch number 763000\n",
      "Training mini-batch number 764000\n",
      "Training mini-batch number 765000\n",
      "Training mini-batch number 766000\n",
      "Training mini-batch number 767000\n",
      "Training mini-batch number 768000\n",
      "Training mini-batch number 769000\n",
      "Training mini-batch number 770000\n",
      "Training mini-batch number 771000\n",
      "Training mini-batch number 772000\n",
      "Training mini-batch number 773000\n",
      "Training mini-batch number 774000\n",
      "Epoch 30: validation accuracy 99.36%\n",
      "Training mini-batch number 775000\n",
      "Training mini-batch number 776000\n",
      "Training mini-batch number 777000\n",
      "Training mini-batch number 778000\n",
      "Training mini-batch number 779000\n",
      "Training mini-batch number 780000\n",
      "Training mini-batch number 781000\n",
      "Training mini-batch number 782000\n",
      "Training mini-batch number 783000\n",
      "Training mini-batch number 784000\n",
      "Training mini-batch number 785000\n",
      "Training mini-batch number 786000\n",
      "Training mini-batch number 787000\n",
      "Training mini-batch number 788000\n",
      "Training mini-batch number 789000\n",
      "Training mini-batch number 790000\n",
      "Training mini-batch number 791000\n",
      "Training mini-batch number 792000\n",
      "Training mini-batch number 793000\n",
      "Training mini-batch number 794000\n",
      "Training mini-batch number 795000\n",
      "Training mini-batch number 796000\n",
      "Training mini-batch number 797000\n",
      "Training mini-batch number 798000\n",
      "Training mini-batch number 799000\n",
      "Epoch 31: validation accuracy 99.38%\n",
      "Training mini-batch number 800000\n",
      "Training mini-batch number 801000\n",
      "Training mini-batch number 802000\n",
      "Training mini-batch number 803000\n",
      "Training mini-batch number 804000\n",
      "Training mini-batch number 805000\n",
      "Training mini-batch number 806000\n",
      "Training mini-batch number 807000\n",
      "Training mini-batch number 808000\n",
      "Training mini-batch number 809000\n",
      "Training mini-batch number 810000\n",
      "Training mini-batch number 811000\n",
      "Training mini-batch number 812000\n",
      "Training mini-batch number 813000\n",
      "Training mini-batch number 814000\n",
      "Training mini-batch number 815000\n",
      "Training mini-batch number 816000\n",
      "Training mini-batch number 817000\n",
      "Training mini-batch number 818000\n",
      "Training mini-batch number 819000\n",
      "Training mini-batch number 820000\n",
      "Training mini-batch number 821000\n",
      "Training mini-batch number 822000\n",
      "Training mini-batch number 823000\n",
      "Training mini-batch number 824000\n",
      "Epoch 32: validation accuracy 99.38%\n",
      "Training mini-batch number 825000\n",
      "Training mini-batch number 826000\n",
      "Training mini-batch number 827000\n",
      "Training mini-batch number 828000\n",
      "Training mini-batch number 829000\n",
      "Training mini-batch number 830000\n",
      "Training mini-batch number 831000\n",
      "Training mini-batch number 832000\n",
      "Training mini-batch number 833000\n",
      "Training mini-batch number 834000\n",
      "Training mini-batch number 835000\n",
      "Training mini-batch number 836000\n",
      "Training mini-batch number 837000\n",
      "Training mini-batch number 838000\n",
      "Training mini-batch number 839000\n",
      "Training mini-batch number 840000\n",
      "Training mini-batch number 841000\n",
      "Training mini-batch number 842000\n",
      "Training mini-batch number 843000\n",
      "Training mini-batch number 844000\n",
      "Training mini-batch number 845000\n",
      "Training mini-batch number 846000\n",
      "Training mini-batch number 847000\n",
      "Training mini-batch number 848000\n",
      "Training mini-batch number 849000\n",
      "Epoch 33: validation accuracy 99.37%\n",
      "Training mini-batch number 850000\n",
      "Training mini-batch number 851000\n",
      "Training mini-batch number 852000\n",
      "Training mini-batch number 853000\n",
      "Training mini-batch number 854000\n",
      "Training mini-batch number 855000\n",
      "Training mini-batch number 856000\n",
      "Training mini-batch number 857000\n",
      "Training mini-batch number 858000\n",
      "Training mini-batch number 859000\n",
      "Training mini-batch number 860000\n",
      "Training mini-batch number 861000\n",
      "Training mini-batch number 862000\n",
      "Training mini-batch number 863000\n",
      "Training mini-batch number 864000\n",
      "Training mini-batch number 865000\n",
      "Training mini-batch number 866000\n",
      "Training mini-batch number 867000\n",
      "Training mini-batch number 868000\n",
      "Training mini-batch number 869000\n",
      "Training mini-batch number 870000\n",
      "Training mini-batch number 871000\n",
      "Training mini-batch number 872000\n",
      "Training mini-batch number 873000\n",
      "Training mini-batch number 874000\n",
      "Epoch 34: validation accuracy 99.38%\n",
      "Training mini-batch number 875000\n",
      "Training mini-batch number 876000\n",
      "Training mini-batch number 877000\n",
      "Training mini-batch number 878000\n",
      "Training mini-batch number 879000\n",
      "Training mini-batch number 880000\n",
      "Training mini-batch number 881000\n",
      "Training mini-batch number 882000\n",
      "Training mini-batch number 883000\n",
      "Training mini-batch number 884000\n",
      "Training mini-batch number 885000\n",
      "Training mini-batch number 886000\n",
      "Training mini-batch number 887000\n",
      "Training mini-batch number 888000\n",
      "Training mini-batch number 889000\n",
      "Training mini-batch number 890000\n",
      "Training mini-batch number 891000\n",
      "Training mini-batch number 892000\n",
      "Training mini-batch number 893000\n",
      "Training mini-batch number 894000\n",
      "Training mini-batch number 895000\n",
      "Training mini-batch number 896000\n",
      "Training mini-batch number 897000\n",
      "Training mini-batch number 898000\n",
      "Training mini-batch number 899000\n",
      "Epoch 35: validation accuracy 99.37%\n",
      "Training mini-batch number 900000\n",
      "Training mini-batch number 901000\n",
      "Training mini-batch number 902000\n",
      "Training mini-batch number 903000\n",
      "Training mini-batch number 904000\n",
      "Training mini-batch number 905000\n",
      "Training mini-batch number 906000\n",
      "Training mini-batch number 907000\n",
      "Training mini-batch number 908000\n",
      "Training mini-batch number 909000\n",
      "Training mini-batch number 910000\n",
      "Training mini-batch number 911000\n",
      "Training mini-batch number 912000\n",
      "Training mini-batch number 913000\n",
      "Training mini-batch number 914000\n",
      "Training mini-batch number 915000\n",
      "Training mini-batch number 916000\n",
      "Training mini-batch number 917000\n",
      "Training mini-batch number 918000\n",
      "Training mini-batch number 919000\n",
      "Training mini-batch number 920000\n",
      "Training mini-batch number 921000\n",
      "Training mini-batch number 922000\n",
      "Training mini-batch number 923000\n",
      "Training mini-batch number 924000\n",
      "Epoch 36: validation accuracy 99.38%\n",
      "Training mini-batch number 925000\n",
      "Training mini-batch number 926000\n",
      "Training mini-batch number 927000\n",
      "Training mini-batch number 928000\n",
      "Training mini-batch number 929000\n",
      "Training mini-batch number 930000\n",
      "Training mini-batch number 931000\n",
      "Training mini-batch number 932000\n",
      "Training mini-batch number 933000\n",
      "Training mini-batch number 934000\n",
      "Training mini-batch number 935000\n",
      "Training mini-batch number 936000\n",
      "Training mini-batch number 937000\n",
      "Training mini-batch number 938000\n",
      "Training mini-batch number 939000\n",
      "Training mini-batch number 940000\n",
      "Training mini-batch number 941000\n",
      "Training mini-batch number 942000\n",
      "Training mini-batch number 943000\n",
      "Training mini-batch number 944000\n",
      "Training mini-batch number 945000\n",
      "Training mini-batch number 946000\n",
      "Training mini-batch number 947000\n",
      "Training mini-batch number 948000\n",
      "Training mini-batch number 949000\n",
      "Epoch 37: validation accuracy 99.39%\n",
      "Training mini-batch number 950000\n",
      "Training mini-batch number 951000\n",
      "Training mini-batch number 952000\n",
      "Training mini-batch number 953000\n",
      "Training mini-batch number 954000\n",
      "Training mini-batch number 955000\n",
      "Training mini-batch number 956000\n",
      "Training mini-batch number 957000\n",
      "Training mini-batch number 958000\n",
      "Training mini-batch number 959000\n",
      "Training mini-batch number 960000\n",
      "Training mini-batch number 961000\n",
      "Training mini-batch number 962000\n",
      "Training mini-batch number 963000\n",
      "Training mini-batch number 964000\n",
      "Training mini-batch number 965000\n",
      "Training mini-batch number 966000\n",
      "Training mini-batch number 967000\n",
      "Training mini-batch number 968000\n",
      "Training mini-batch number 969000\n",
      "Training mini-batch number 970000\n",
      "Training mini-batch number 971000\n",
      "Training mini-batch number 972000\n",
      "Training mini-batch number 973000\n",
      "Training mini-batch number 974000\n",
      "Epoch 38: validation accuracy 99.38%\n",
      "Training mini-batch number 975000\n",
      "Training mini-batch number 976000\n",
      "Training mini-batch number 977000\n",
      "Training mini-batch number 978000\n",
      "Training mini-batch number 979000\n",
      "Training mini-batch number 980000\n",
      "Training mini-batch number 981000\n",
      "Training mini-batch number 982000\n",
      "Training mini-batch number 983000\n",
      "Training mini-batch number 984000\n",
      "Training mini-batch number 985000\n",
      "Training mini-batch number 986000\n",
      "Training mini-batch number 987000\n",
      "Training mini-batch number 988000\n",
      "Training mini-batch number 989000\n",
      "Training mini-batch number 990000\n",
      "Training mini-batch number 991000\n",
      "Training mini-batch number 992000\n",
      "Training mini-batch number 993000\n",
      "Training mini-batch number 994000\n",
      "Training mini-batch number 995000\n",
      "Training mini-batch number 996000\n",
      "Training mini-batch number 997000\n",
      "Training mini-batch number 998000\n",
      "Training mini-batch number 999000\n",
      "Epoch 39: validation accuracy 99.38%\n",
      "Training mini-batch number 1000000\n",
      "Training mini-batch number 1001000\n",
      "Training mini-batch number 1002000\n",
      "Training mini-batch number 1003000\n",
      "Training mini-batch number 1004000\n",
      "Training mini-batch number 1005000\n",
      "Training mini-batch number 1006000\n",
      "Training mini-batch number 1007000\n",
      "Training mini-batch number 1008000\n",
      "Training mini-batch number 1009000\n",
      "Training mini-batch number 1010000\n",
      "Training mini-batch number 1011000\n",
      "Training mini-batch number 1012000\n",
      "Training mini-batch number 1013000\n",
      "Training mini-batch number 1014000\n",
      "Training mini-batch number 1015000\n",
      "Training mini-batch number 1016000\n",
      "Training mini-batch number 1017000\n",
      "Training mini-batch number 1018000\n",
      "Training mini-batch number 1019000\n",
      "Training mini-batch number 1020000\n",
      "Training mini-batch number 1021000\n",
      "Training mini-batch number 1022000\n",
      "Training mini-batch number 1023000\n",
      "Training mini-batch number 1024000\n",
      "Epoch 40: validation accuracy 99.37%\n",
      "Training mini-batch number 1025000\n",
      "Training mini-batch number 1026000\n",
      "Training mini-batch number 1027000\n",
      "Training mini-batch number 1028000\n",
      "Training mini-batch number 1029000\n",
      "Training mini-batch number 1030000\n",
      "Training mini-batch number 1031000\n",
      "Training mini-batch number 1032000\n",
      "Training mini-batch number 1033000\n",
      "Training mini-batch number 1034000\n",
      "Training mini-batch number 1035000\n",
      "Training mini-batch number 1036000\n",
      "Training mini-batch number 1037000\n",
      "Training mini-batch number 1038000\n",
      "Training mini-batch number 1039000\n",
      "Training mini-batch number 1040000\n",
      "Training mini-batch number 1041000\n",
      "Training mini-batch number 1042000\n",
      "Training mini-batch number 1043000\n",
      "Training mini-batch number 1044000\n",
      "Training mini-batch number 1045000\n",
      "Training mini-batch number 1046000\n",
      "Training mini-batch number 1047000\n",
      "Training mini-batch number 1048000\n",
      "Training mini-batch number 1049000\n",
      "Epoch 41: validation accuracy 99.37%\n",
      "Training mini-batch number 1050000\n",
      "Training mini-batch number 1051000\n",
      "Training mini-batch number 1052000\n",
      "Training mini-batch number 1053000\n",
      "Training mini-batch number 1054000\n",
      "Training mini-batch number 1055000\n",
      "Training mini-batch number 1056000\n",
      "Training mini-batch number 1057000\n",
      "Training mini-batch number 1058000\n",
      "Training mini-batch number 1059000\n",
      "Training mini-batch number 1060000\n",
      "Training mini-batch number 1061000\n",
      "Training mini-batch number 1062000\n",
      "Training mini-batch number 1063000\n",
      "Training mini-batch number 1064000\n",
      "Training mini-batch number 1065000\n",
      "Training mini-batch number 1066000\n",
      "Training mini-batch number 1067000\n",
      "Training mini-batch number 1068000\n",
      "Training mini-batch number 1069000\n",
      "Training mini-batch number 1070000\n",
      "Training mini-batch number 1071000\n",
      "Training mini-batch number 1072000\n",
      "Training mini-batch number 1073000\n",
      "Training mini-batch number 1074000\n",
      "Epoch 42: validation accuracy 99.37%\n",
      "Training mini-batch number 1075000\n",
      "Training mini-batch number 1076000\n",
      "Training mini-batch number 1077000\n",
      "Training mini-batch number 1078000\n",
      "Training mini-batch number 1079000\n",
      "Training mini-batch number 1080000\n",
      "Training mini-batch number 1081000\n",
      "Training mini-batch number 1082000\n",
      "Training mini-batch number 1083000\n",
      "Training mini-batch number 1084000\n",
      "Training mini-batch number 1085000\n",
      "Training mini-batch number 1086000\n",
      "Training mini-batch number 1087000\n",
      "Training mini-batch number 1088000\n",
      "Training mini-batch number 1089000\n",
      "Training mini-batch number 1090000\n",
      "Training mini-batch number 1091000\n",
      "Training mini-batch number 1092000\n",
      "Training mini-batch number 1093000\n",
      "Training mini-batch number 1094000\n",
      "Training mini-batch number 1095000\n",
      "Training mini-batch number 1096000\n",
      "Training mini-batch number 1097000\n",
      "Training mini-batch number 1098000\n",
      "Training mini-batch number 1099000\n",
      "Epoch 43: validation accuracy 99.36%\n",
      "Training mini-batch number 1100000\n",
      "Training mini-batch number 1101000\n",
      "Training mini-batch number 1102000\n",
      "Training mini-batch number 1103000\n",
      "Training mini-batch number 1104000\n",
      "Training mini-batch number 1105000\n",
      "Training mini-batch number 1106000\n",
      "Training mini-batch number 1107000\n",
      "Training mini-batch number 1108000\n",
      "Training mini-batch number 1109000\n",
      "Training mini-batch number 1110000\n",
      "Training mini-batch number 1111000\n",
      "Training mini-batch number 1112000\n",
      "Training mini-batch number 1113000\n",
      "Training mini-batch number 1114000\n",
      "Training mini-batch number 1115000\n",
      "Training mini-batch number 1116000\n",
      "Training mini-batch number 1117000\n",
      "Training mini-batch number 1118000\n",
      "Training mini-batch number 1119000\n",
      "Training mini-batch number 1120000\n",
      "Training mini-batch number 1121000\n",
      "Training mini-batch number 1122000\n",
      "Training mini-batch number 1123000\n",
      "Training mini-batch number 1124000\n",
      "Epoch 44: validation accuracy 99.36%\n",
      "Training mini-batch number 1125000\n",
      "Training mini-batch number 1126000\n",
      "Training mini-batch number 1127000\n",
      "Training mini-batch number 1128000\n",
      "Training mini-batch number 1129000\n",
      "Training mini-batch number 1130000\n",
      "Training mini-batch number 1131000\n",
      "Training mini-batch number 1132000\n",
      "Training mini-batch number 1133000\n",
      "Training mini-batch number 1134000\n",
      "Training mini-batch number 1135000\n",
      "Training mini-batch number 1136000\n",
      "Training mini-batch number 1137000\n",
      "Training mini-batch number 1138000\n",
      "Training mini-batch number 1139000\n",
      "Training mini-batch number 1140000\n",
      "Training mini-batch number 1141000\n",
      "Training mini-batch number 1142000\n",
      "Training mini-batch number 1143000\n",
      "Training mini-batch number 1144000\n",
      "Training mini-batch number 1145000\n",
      "Training mini-batch number 1146000\n",
      "Training mini-batch number 1147000\n",
      "Training mini-batch number 1148000\n",
      "Training mini-batch number 1149000\n",
      "Epoch 45: validation accuracy 99.37%\n",
      "Training mini-batch number 1150000\n",
      "Training mini-batch number 1151000\n",
      "Training mini-batch number 1152000\n",
      "Training mini-batch number 1153000\n",
      "Training mini-batch number 1154000\n",
      "Training mini-batch number 1155000\n",
      "Training mini-batch number 1156000\n",
      "Training mini-batch number 1157000\n",
      "Training mini-batch number 1158000\n",
      "Training mini-batch number 1159000\n",
      "Training mini-batch number 1160000\n",
      "Training mini-batch number 1161000\n",
      "Training mini-batch number 1162000\n",
      "Training mini-batch number 1163000\n",
      "Training mini-batch number 1164000\n",
      "Training mini-batch number 1165000\n",
      "Training mini-batch number 1166000\n",
      "Training mini-batch number 1167000\n",
      "Training mini-batch number 1168000\n",
      "Training mini-batch number 1169000\n",
      "Training mini-batch number 1170000\n",
      "Training mini-batch number 1171000\n",
      "Training mini-batch number 1172000\n",
      "Training mini-batch number 1173000\n",
      "Training mini-batch number 1174000\n",
      "Epoch 46: validation accuracy 99.36%\n",
      "Training mini-batch number 1175000\n",
      "Training mini-batch number 1176000\n",
      "Training mini-batch number 1177000\n",
      "Training mini-batch number 1178000\n",
      "Training mini-batch number 1179000\n",
      "Training mini-batch number 1180000\n",
      "Training mini-batch number 1181000\n",
      "Training mini-batch number 1182000\n",
      "Training mini-batch number 1183000\n",
      "Training mini-batch number 1184000\n",
      "Training mini-batch number 1185000\n",
      "Training mini-batch number 1186000\n",
      "Training mini-batch number 1187000\n",
      "Training mini-batch number 1188000\n",
      "Training mini-batch number 1189000\n",
      "Training mini-batch number 1190000\n",
      "Training mini-batch number 1191000\n",
      "Training mini-batch number 1192000\n",
      "Training mini-batch number 1193000\n",
      "Training mini-batch number 1194000\n",
      "Training mini-batch number 1195000\n",
      "Training mini-batch number 1196000\n",
      "Training mini-batch number 1197000\n",
      "Training mini-batch number 1198000\n",
      "Training mini-batch number 1199000\n",
      "Epoch 47: validation accuracy 99.37%\n",
      "Training mini-batch number 1200000\n",
      "Training mini-batch number 1201000\n",
      "Training mini-batch number 1202000\n",
      "Training mini-batch number 1203000\n",
      "Training mini-batch number 1204000\n",
      "Training mini-batch number 1205000\n",
      "Training mini-batch number 1206000\n",
      "Training mini-batch number 1207000\n",
      "Training mini-batch number 1208000\n",
      "Training mini-batch number 1209000\n",
      "Training mini-batch number 1210000\n",
      "Training mini-batch number 1211000\n",
      "Training mini-batch number 1212000\n",
      "Training mini-batch number 1213000\n",
      "Training mini-batch number 1214000\n",
      "Training mini-batch number 1215000\n",
      "Training mini-batch number 1216000\n",
      "Training mini-batch number 1217000\n",
      "Training mini-batch number 1218000\n",
      "Training mini-batch number 1219000\n",
      "Training mini-batch number 1220000\n",
      "Training mini-batch number 1221000\n",
      "Training mini-batch number 1222000\n",
      "Training mini-batch number 1223000\n",
      "Training mini-batch number 1224000\n",
      "Epoch 48: validation accuracy 99.37%\n",
      "Training mini-batch number 1225000\n",
      "Training mini-batch number 1226000\n",
      "Training mini-batch number 1227000\n",
      "Training mini-batch number 1228000\n",
      "Training mini-batch number 1229000\n",
      "Training mini-batch number 1230000\n",
      "Training mini-batch number 1231000\n",
      "Training mini-batch number 1232000\n",
      "Training mini-batch number 1233000\n",
      "Training mini-batch number 1234000\n",
      "Training mini-batch number 1235000\n",
      "Training mini-batch number 1236000\n",
      "Training mini-batch number 1237000\n",
      "Training mini-batch number 1238000\n",
      "Training mini-batch number 1239000\n",
      "Training mini-batch number 1240000\n",
      "Training mini-batch number 1241000\n",
      "Training mini-batch number 1242000\n",
      "Training mini-batch number 1243000\n",
      "Training mini-batch number 1244000\n",
      "Training mini-batch number 1245000\n",
      "Training mini-batch number 1246000\n",
      "Training mini-batch number 1247000\n",
      "Training mini-batch number 1248000\n",
      "Training mini-batch number 1249000\n",
      "Epoch 49: validation accuracy 99.37%\n",
      "Training mini-batch number 1250000\n",
      "Training mini-batch number 1251000\n",
      "Training mini-batch number 1252000\n",
      "Training mini-batch number 1253000\n",
      "Training mini-batch number 1254000\n",
      "Training mini-batch number 1255000\n",
      "Training mini-batch number 1256000\n",
      "Training mini-batch number 1257000\n",
      "Training mini-batch number 1258000\n",
      "Training mini-batch number 1259000\n",
      "Training mini-batch number 1260000\n",
      "Training mini-batch number 1261000\n",
      "Training mini-batch number 1262000\n",
      "Training mini-batch number 1263000\n",
      "Training mini-batch number 1264000\n",
      "Training mini-batch number 1265000\n",
      "Training mini-batch number 1266000\n",
      "Training mini-batch number 1267000\n",
      "Training mini-batch number 1268000\n",
      "Training mini-batch number 1269000\n",
      "Training mini-batch number 1270000\n",
      "Training mini-batch number 1271000\n",
      "Training mini-batch number 1272000\n",
      "Training mini-batch number 1273000\n",
      "Training mini-batch number 1274000\n",
      "Epoch 50: validation accuracy 99.36%\n",
      "Training mini-batch number 1275000\n",
      "Training mini-batch number 1276000\n",
      "Training mini-batch number 1277000\n",
      "Training mini-batch number 1278000\n",
      "Training mini-batch number 1279000\n",
      "Training mini-batch number 1280000\n",
      "Training mini-batch number 1281000\n",
      "Training mini-batch number 1282000\n",
      "Training mini-batch number 1283000\n",
      "Training mini-batch number 1284000\n",
      "Training mini-batch number 1285000\n",
      "Training mini-batch number 1286000\n",
      "Training mini-batch number 1287000\n",
      "Training mini-batch number 1288000\n",
      "Training mini-batch number 1289000\n",
      "Training mini-batch number 1290000\n",
      "Training mini-batch number 1291000\n",
      "Training mini-batch number 1292000\n",
      "Training mini-batch number 1293000\n",
      "Training mini-batch number 1294000\n",
      "Training mini-batch number 1295000\n",
      "Training mini-batch number 1296000\n",
      "Training mini-batch number 1297000\n",
      "Training mini-batch number 1298000\n",
      "Training mini-batch number 1299000\n",
      "Epoch 51: validation accuracy 99.36%\n",
      "Training mini-batch number 1300000\n",
      "Training mini-batch number 1301000\n",
      "Training mini-batch number 1302000\n",
      "Training mini-batch number 1303000\n",
      "Training mini-batch number 1304000\n",
      "Training mini-batch number 1305000\n",
      "Training mini-batch number 1306000\n",
      "Training mini-batch number 1307000\n",
      "Training mini-batch number 1308000\n",
      "Training mini-batch number 1309000\n",
      "Training mini-batch number 1310000\n",
      "Training mini-batch number 1311000\n",
      "Training mini-batch number 1312000\n",
      "Training mini-batch number 1313000\n",
      "Training mini-batch number 1314000\n",
      "Training mini-batch number 1315000\n",
      "Training mini-batch number 1316000\n",
      "Training mini-batch number 1317000\n",
      "Training mini-batch number 1318000\n",
      "Training mini-batch number 1319000\n",
      "Training mini-batch number 1320000\n",
      "Training mini-batch number 1321000\n",
      "Training mini-batch number 1322000\n",
      "Training mini-batch number 1323000\n",
      "Training mini-batch number 1324000\n",
      "Epoch 52: validation accuracy 99.37%\n",
      "Training mini-batch number 1325000\n",
      "Training mini-batch number 1326000\n",
      "Training mini-batch number 1327000\n",
      "Training mini-batch number 1328000\n",
      "Training mini-batch number 1329000\n",
      "Training mini-batch number 1330000\n",
      "Training mini-batch number 1331000\n",
      "Training mini-batch number 1332000\n",
      "Training mini-batch number 1333000\n",
      "Training mini-batch number 1334000\n",
      "Training mini-batch number 1335000\n",
      "Training mini-batch number 1336000\n",
      "Training mini-batch number 1337000\n",
      "Training mini-batch number 1338000\n",
      "Training mini-batch number 1339000\n",
      "Training mini-batch number 1340000\n",
      "Training mini-batch number 1341000\n",
      "Training mini-batch number 1342000\n",
      "Training mini-batch number 1343000\n",
      "Training mini-batch number 1344000\n",
      "Training mini-batch number 1345000\n",
      "Training mini-batch number 1346000\n",
      "Training mini-batch number 1347000\n",
      "Training mini-batch number 1348000\n",
      "Training mini-batch number 1349000\n",
      "Epoch 53: validation accuracy 99.37%\n",
      "Training mini-batch number 1350000\n",
      "Training mini-batch number 1351000\n",
      "Training mini-batch number 1352000\n",
      "Training mini-batch number 1353000\n",
      "Training mini-batch number 1354000\n",
      "Training mini-batch number 1355000\n",
      "Training mini-batch number 1356000\n",
      "Training mini-batch number 1357000\n",
      "Training mini-batch number 1358000\n",
      "Training mini-batch number 1359000\n",
      "Training mini-batch number 1360000\n",
      "Training mini-batch number 1361000\n",
      "Training mini-batch number 1362000\n",
      "Training mini-batch number 1363000\n",
      "Training mini-batch number 1364000\n",
      "Training mini-batch number 1365000\n",
      "Training mini-batch number 1366000\n",
      "Training mini-batch number 1367000\n",
      "Training mini-batch number 1368000\n",
      "Training mini-batch number 1369000\n",
      "Training mini-batch number 1370000\n",
      "Training mini-batch number 1371000\n",
      "Training mini-batch number 1372000\n",
      "Training mini-batch number 1373000\n",
      "Training mini-batch number 1374000\n",
      "Epoch 54: validation accuracy 99.38%\n",
      "Training mini-batch number 1375000\n",
      "Training mini-batch number 1376000\n",
      "Training mini-batch number 1377000\n",
      "Training mini-batch number 1378000\n",
      "Training mini-batch number 1379000\n",
      "Training mini-batch number 1380000\n",
      "Training mini-batch number 1381000\n",
      "Training mini-batch number 1382000\n",
      "Training mini-batch number 1383000\n",
      "Training mini-batch number 1384000\n",
      "Training mini-batch number 1385000\n",
      "Training mini-batch number 1386000\n",
      "Training mini-batch number 1387000\n",
      "Training mini-batch number 1388000\n",
      "Training mini-batch number 1389000\n",
      "Training mini-batch number 1390000\n",
      "Training mini-batch number 1391000\n",
      "Training mini-batch number 1392000\n",
      "Training mini-batch number 1393000\n",
      "Training mini-batch number 1394000\n",
      "Training mini-batch number 1395000\n",
      "Training mini-batch number 1396000\n",
      "Training mini-batch number 1397000\n",
      "Training mini-batch number 1398000\n",
      "Training mini-batch number 1399000\n",
      "Epoch 55: validation accuracy 99.38%\n",
      "Training mini-batch number 1400000\n",
      "Training mini-batch number 1401000\n",
      "Training mini-batch number 1402000\n",
      "Training mini-batch number 1403000\n",
      "Training mini-batch number 1404000\n",
      "Training mini-batch number 1405000\n",
      "Training mini-batch number 1406000\n",
      "Training mini-batch number 1407000\n",
      "Training mini-batch number 1408000\n",
      "Training mini-batch number 1409000\n",
      "Training mini-batch number 1410000\n",
      "Training mini-batch number 1411000\n",
      "Training mini-batch number 1412000\n",
      "Training mini-batch number 1413000\n",
      "Training mini-batch number 1414000\n",
      "Training mini-batch number 1415000\n",
      "Training mini-batch number 1416000\n",
      "Training mini-batch number 1417000\n",
      "Training mini-batch number 1418000\n",
      "Training mini-batch number 1419000\n",
      "Training mini-batch number 1420000\n",
      "Training mini-batch number 1421000\n",
      "Training mini-batch number 1422000\n",
      "Training mini-batch number 1423000\n",
      "Training mini-batch number 1424000\n",
      "Epoch 56: validation accuracy 99.38%\n",
      "Training mini-batch number 1425000\n",
      "Training mini-batch number 1426000\n",
      "Training mini-batch number 1427000\n",
      "Training mini-batch number 1428000\n",
      "Training mini-batch number 1429000\n",
      "Training mini-batch number 1430000\n",
      "Training mini-batch number 1431000\n",
      "Training mini-batch number 1432000\n",
      "Training mini-batch number 1433000\n",
      "Training mini-batch number 1434000\n",
      "Training mini-batch number 1435000\n",
      "Training mini-batch number 1436000\n",
      "Training mini-batch number 1437000\n",
      "Training mini-batch number 1438000\n",
      "Training mini-batch number 1439000\n",
      "Training mini-batch number 1440000\n",
      "Training mini-batch number 1441000\n",
      "Training mini-batch number 1442000\n",
      "Training mini-batch number 1443000\n",
      "Training mini-batch number 1444000\n",
      "Training mini-batch number 1445000\n",
      "Training mini-batch number 1446000\n",
      "Training mini-batch number 1447000\n",
      "Training mini-batch number 1448000\n",
      "Training mini-batch number 1449000\n",
      "Epoch 57: validation accuracy 99.38%\n",
      "Training mini-batch number 1450000\n",
      "Training mini-batch number 1451000\n",
      "Training mini-batch number 1452000\n",
      "Training mini-batch number 1453000\n",
      "Training mini-batch number 1454000\n",
      "Training mini-batch number 1455000\n",
      "Training mini-batch number 1456000\n",
      "Training mini-batch number 1457000\n",
      "Training mini-batch number 1458000\n",
      "Training mini-batch number 1459000\n",
      "Training mini-batch number 1460000\n",
      "Training mini-batch number 1461000\n",
      "Training mini-batch number 1462000\n",
      "Training mini-batch number 1463000\n",
      "Training mini-batch number 1464000\n",
      "Training mini-batch number 1465000\n",
      "Training mini-batch number 1466000\n",
      "Training mini-batch number 1467000\n",
      "Training mini-batch number 1468000\n",
      "Training mini-batch number 1469000\n",
      "Training mini-batch number 1470000\n",
      "Training mini-batch number 1471000\n",
      "Training mini-batch number 1472000\n",
      "Training mini-batch number 1473000\n",
      "Training mini-batch number 1474000\n",
      "Epoch 58: validation accuracy 99.38%\n",
      "Training mini-batch number 1475000\n",
      "Training mini-batch number 1476000\n",
      "Training mini-batch number 1477000\n",
      "Training mini-batch number 1478000\n",
      "Training mini-batch number 1479000\n",
      "Training mini-batch number 1480000\n",
      "Training mini-batch number 1481000\n",
      "Training mini-batch number 1482000\n",
      "Training mini-batch number 1483000\n",
      "Training mini-batch number 1484000\n",
      "Training mini-batch number 1485000\n",
      "Training mini-batch number 1486000\n",
      "Training mini-batch number 1487000\n",
      "Training mini-batch number 1488000\n",
      "Training mini-batch number 1489000\n",
      "Training mini-batch number 1490000\n",
      "Training mini-batch number 1491000\n",
      "Training mini-batch number 1492000\n",
      "Training mini-batch number 1493000\n",
      "Training mini-batch number 1494000\n",
      "Training mini-batch number 1495000\n",
      "Training mini-batch number 1496000\n",
      "Training mini-batch number 1497000\n",
      "Training mini-batch number 1498000\n",
      "Training mini-batch number 1499000\n",
      "Epoch 59: validation accuracy 99.38%\n",
      "Finished training network.\n",
      "Best validation accuracy of 99.40% obtained at iteration 499999\n",
      "Corresponding test accuracy of 99.33%\n"
     ]
    }
   ],
   "source": [
    "net.SGD(expanded_training_data, 60, mini_batch_size, 0.03, \n",
    "            validation_data, test_data, lmbda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
    "                     filter_shape=(20, 1, 5, 5),\n",
    "                     poolsize=(2, 2),\n",
    "                     activation_fn=ReLU),\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
    "                     filter_shape=(40, 20, 5, 5),\n",
    "                     poolsize=(2, 2),\n",
    "                     activation_fn=ReLU),\n",
    "        FullyConnectedLayer(\n",
    "            n_in=40*4*4, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
    "        FullyConnectedLayer(\n",
    "            n_in=1000, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
    "        SoftmaxLayer(n_in=1000, n_out=10, p_dropout=0.5)],\n",
    "        mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Rebroadcast{?,0}.0) of Op Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Subtensor{::, ::, ::int64, ::int64}.0) of Op Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Rebroadcast{?,0}.0) of Op Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Subtensor{::, ::, ::int64, ::int64}.0) of Op Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Rebroadcast{?,0}.0) of Op Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Subtensor{::, ::, ::int64, ::int64}.0) of Op Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
      "    c = node.op(*inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
      "    new = self.merge_num_denum(num, denum)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
      "    return self.main(*num)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(Elemwise{mul,no_inplace}.0, w)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op Dot22(Elemwise{mul,no_inplace}.0, w) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(Elemwise{mul,no_inplace}.0, w)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op Dot22(Elemwise{mul,no_inplace}.0, w) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(Elemwise{mul,no_inplace}.0, w)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op Dot22(Elemwise{mul,no_inplace}.0, w) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(DimShuffle{1,0}.0, SoftmaxGrad.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (DimShuffle{1,0}.0) of Op Dot22(DimShuffle{1,0}.0, SoftmaxGrad.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(SoftmaxGrad.0, w.T)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (SoftmaxGrad.0) of Op Dot22(SoftmaxGrad.0, w.T) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(DimShuffle{1,0}.0, Elemwise{mul}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (DimShuffle{1,0}.0) of Op Dot22(DimShuffle{1,0}.0, Elemwise{mul}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(Elemwise{mul}.0, w.T)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul}.0) of Op Dot22(Elemwise{mul}.0, w.T) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(DimShuffle{1,0}.0, Elemwise{mul}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (DimShuffle{1,0}.0) of Op Dot22(DimShuffle{1,0}.0, Elemwise{mul}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
      "ERROR (theano.gof.opt): node: dot(Elemwise{mul}.0, w.T)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
      "    return [_dot22(*node.inputs)]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul}.0) of Op Dot22(Elemwise{mul}.0, w.T) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
      "    ret = fill_chain(new_inputs[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
      "    out = _fill_chain(v, node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
      "    new_out = T.fill(i, new_out)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Rebroadcast{?,0}.0) of Op Elemwise{second,no_inplace}(Rebroadcast{?,0}.0, Rebroadcast{?,0}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
      "    ret = fill_chain(new_inputs[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
      "    out = _fill_chain(v, node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
      "    new_out = T.fill(i, new_out)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Subtensor{::, ::, ::int64, ::int64}.0) of Op Elemwise{second,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Subtensor{::, ::, ::int64, ::int64}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
      "    ret = fill_chain(new_inputs[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
      "    out = _fill_chain(v, node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
      "    new_out = T.fill(i, new_out)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{second,no_inplace}(dot.0, dot.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
      "    ret = fill_chain(new_inputs[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
      "    out = _fill_chain(v, node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
      "    new_out = T.fill(i, new_out)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{second,no_inplace}(dot.0, dot.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
      "    ret = fill_chain(new_inputs[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
      "    out = _fill_chain(v, node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
      "    new_out = T.fill(i, new_out)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{second,no_inplace}(dot.0, dot.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_advanced_indexing_crossentropy_onehot_grad\n",
      "ERROR (theano.gof.opt): node: SoftmaxGrad(Elemwise{true_div}.0, Softmax.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 1907, in local_advanced_indexing_crossentropy_onehot_grad\n",
      "    out_grad = -incr\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 39, in __neg__\n",
      "    return theano.tensor.basic.neg(self)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{second}.0) of Op Elemwise{neg,no_inplace}(Elemwise{second}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_mul_specialize\n",
      "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(TensorConstant{-1.0}, mean)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5427, in local_mul_specialize\n",
      "    rval = -new_inputs[0]\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 39, in __neg__\n",
      "    return theano.tensor.basic.neg(self)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (mean) of Op Elemwise{neg,no_inplace}(mean) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_advanced_indexing_crossentropy_onehot\n",
      "ERROR (theano.gof.opt): node: AdvancedSubtensor(Elemwise{log,no_inplace}.0, ARange{dtype='int64'}.0, Elemwise{Cast{int32}}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 1692, in local_advanced_indexing_crossentropy_onehot\n",
      "    b_var = tensor.zeros_like(x_var[0])\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 532, in __getitem__\n",
      "    lambda entry: isinstance(entry, Variable)))\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{add,no_inplace}.0) of Op Subtensor{int64}(Elemwise{add,no_inplace}.0, Constant{0}) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_logsoftmax\n",
      "ERROR (theano.gof.opt): node: Elemwise{log,no_inplace}(Softmax.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 756, in local_logsoftmax\n",
      "    ret = new_op(inVars)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{add,no_inplace}.0) of Op LogSoftmax(Elemwise{add,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
      "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
      "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
      "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
      "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): SeqOptimizer apply <theano.tensor.opt.FusionOptimizer object at 0x7f862ca00ad0>\n",
      "ERROR (theano.gof.opt): Traceback:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 230, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 89, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 6658, in apply\n",
      "    new_outputs = self.optimizer(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 6502, in local_fuse\n",
      "    return_list=True)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 1 (<float64>) of Op mul(<float64>, <float64>) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): SeqOptimizer apply <theano.compile.mode.AddDestroyHandler object at 0x7f8649eada50>\n",
      "ERROR (theano.gof.opt): Traceback:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 230, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 89, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/compile/mode.py\", line 134, in apply\n",
      "    fgraph.replace_validate(o, _output_guard(o),\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op OutputGuard(Elemwise{mul,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(Reshape{4}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Reshape{4}.0) of Op InplaceDimShuffle{1,0,2,3}(Reshape{4}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(Reshape{4}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Reshape{4}.0) of Op InplaceDimShuffle{1,0,2,3}(Reshape{4}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{x}(Elemwise{Cast{float64}}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{Cast{float64}}.0) of Op InplaceDimShuffle{x}(Elemwise{Cast{float64}}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0}(Elemwise{mul,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0}(Elemwise{mul,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0}(Elemwise{mul,no_inplace}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0) of Op InplaceDimShuffle{1,0,2,3}(MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(ConvOp{('imshp', (10, 12, 12)),('kshp', (8, 8)),('nkern', 40),('bsize', 20),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (10, 12, 12)),('kshp_logical', (8, 8)),('kshp_logical_top_aligned', False)}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (ConvOp{('imshp', (10, 12, 12)),('kshp', (8, 8)),('nkern', 40),('bsize', 20),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (10, 12, 12)),('kshp_logical', (8, 8)),('kshp_logical_top_aligned', False)}.0) of Op InplaceDimShuffle{1,0,2,3}(ConvOp{('imshp', (10, 12, 12)),('kshp', (8, 8)),('nkern', 40),('bsize', 20),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (10, 12, 12)),('kshp_logical', (8, 8)),('kshp_logical_top_aligned', False)}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0) of Op InplaceDimShuffle{1,0,2,3}(MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0) missing default value\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
      "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(ConvOp{('imshp', (10, 28, 28)),('kshp', (24, 24)),('nkern', 20),('bsize', 1),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 1),('unroll_kern', 10),('unroll_patch', False),('imshp_logical', (10, 28, 28)),('kshp_logical', (24, 24)),('kshp_logical_top_aligned', False)}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
      "    v = new_op(*node.inputs)\n",
      "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
      "    (i, ins, node))\n",
      "ValueError: Cannot compute test value: input 0 (ConvOp{('imshp', (10, 28, 28)),('kshp', (24, 24)),('nkern', 20),('bsize', 1),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 1),('unroll_kern', 10),('unroll_patch', False),('imshp_logical', (10, 28, 28)),('kshp_logical', (24, 24)),('kshp_logical_top_aligned', False)}.0) of Op InplaceDimShuffle{1,0,2,3}(ConvOp{('imshp', (10, 28, 28)),('kshp', (24, 24)),('nkern', 20),('bsize', 1),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 1),('unroll_kern', 10),('unroll_patch', False),('imshp_logical', (10, 28, 28)),('kshp_logical', (24, 24)),('kshp_logical_top_aligned', False)}.0) missing default value\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 0: validation accuracy 98.63%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.81%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 1: validation accuracy 99.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.07%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 2: validation accuracy 99.17%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.22%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 3: validation accuracy 99.25%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.33%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 4: validation accuracy 99.29%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.44%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 5: validation accuracy 99.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.47%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 6: validation accuracy 99.39%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.54%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 7: validation accuracy 99.37%\n",
      "Training mini-batch number 200000\n",
      "Training mini-batch number 201000\n",
      "Training mini-batch number 202000\n",
      "Training mini-batch number 203000\n",
      "Training mini-batch number 204000\n",
      "Training mini-batch number 205000\n",
      "Training mini-batch number 206000\n",
      "Training mini-batch number 207000\n",
      "Training mini-batch number 208000\n",
      "Training mini-batch number 209000\n",
      "Training mini-batch number 210000\n",
      "Training mini-batch number 211000\n",
      "Training mini-batch number 212000\n",
      "Training mini-batch number 213000\n",
      "Training mini-batch number 214000\n",
      "Training mini-batch number 215000\n",
      "Training mini-batch number 216000\n",
      "Training mini-batch number 217000\n",
      "Training mini-batch number 218000\n",
      "Training mini-batch number 219000\n",
      "Training mini-batch number 220000\n",
      "Training mini-batch number 221000\n",
      "Training mini-batch number 222000\n",
      "Training mini-batch number 223000\n",
      "Training mini-batch number 224000\n",
      "Epoch 8: validation accuracy 99.40%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.47%\n",
      "Training mini-batch number 225000\n",
      "Training mini-batch number 226000\n",
      "Training mini-batch number 227000\n",
      "Training mini-batch number 228000\n",
      "Training mini-batch number 229000\n",
      "Training mini-batch number 230000\n",
      "Training mini-batch number 231000\n",
      "Training mini-batch number 232000\n",
      "Training mini-batch number 233000\n",
      "Training mini-batch number 234000\n",
      "Training mini-batch number 235000\n",
      "Training mini-batch number 236000\n",
      "Training mini-batch number 237000\n",
      "Training mini-batch number 238000\n",
      "Training mini-batch number 239000\n",
      "Training mini-batch number 240000\n",
      "Training mini-batch number 241000\n",
      "Training mini-batch number 242000\n",
      "Training mini-batch number 243000\n",
      "Training mini-batch number 244000\n",
      "Training mini-batch number 245000\n",
      "Training mini-batch number 246000\n",
      "Training mini-batch number 247000\n",
      "Training mini-batch number 248000\n",
      "Training mini-batch number 249000\n",
      "Epoch 9: validation accuracy 99.46%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.51%\n",
      "Training mini-batch number 250000\n",
      "Training mini-batch number 251000\n",
      "Training mini-batch number 252000\n",
      "Training mini-batch number 253000\n",
      "Training mini-batch number 254000\n",
      "Training mini-batch number 255000\n",
      "Training mini-batch number 256000\n",
      "Training mini-batch number 257000\n",
      "Training mini-batch number 258000\n",
      "Training mini-batch number 259000\n",
      "Training mini-batch number 260000\n",
      "Training mini-batch number 261000\n",
      "Training mini-batch number 262000\n",
      "Training mini-batch number 263000\n",
      "Training mini-batch number 264000\n",
      "Training mini-batch number 265000\n",
      "Training mini-batch number 266000\n",
      "Training mini-batch number 267000\n",
      "Training mini-batch number 268000\n",
      "Training mini-batch number 269000\n",
      "Training mini-batch number 270000\n",
      "Training mini-batch number 271000\n",
      "Training mini-batch number 272000\n",
      "Training mini-batch number 273000\n",
      "Training mini-batch number 274000\n",
      "Epoch 10: validation accuracy 99.49%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.57%\n",
      "Training mini-batch number 275000\n",
      "Training mini-batch number 276000\n",
      "Training mini-batch number 277000\n",
      "Training mini-batch number 278000\n",
      "Training mini-batch number 279000\n",
      "Training mini-batch number 280000\n",
      "Training mini-batch number 281000\n",
      "Training mini-batch number 282000\n",
      "Training mini-batch number 283000\n",
      "Training mini-batch number 284000\n",
      "Training mini-batch number 285000\n",
      "Training mini-batch number 286000\n",
      "Training mini-batch number 287000\n",
      "Training mini-batch number 288000\n",
      "Training mini-batch number 289000\n",
      "Training mini-batch number 290000\n",
      "Training mini-batch number 291000\n",
      "Training mini-batch number 292000\n",
      "Training mini-batch number 293000\n",
      "Training mini-batch number 294000\n",
      "Training mini-batch number 295000\n",
      "Training mini-batch number 296000\n",
      "Training mini-batch number 297000\n",
      "Training mini-batch number 298000\n",
      "Training mini-batch number 299000\n",
      "Epoch 11: validation accuracy 99.40%\n",
      "Training mini-batch number 300000\n",
      "Training mini-batch number 301000\n",
      "Training mini-batch number 302000\n",
      "Training mini-batch number 303000\n",
      "Training mini-batch number 304000\n",
      "Training mini-batch number 305000\n",
      "Training mini-batch number 306000\n",
      "Training mini-batch number 307000\n",
      "Training mini-batch number 308000\n",
      "Training mini-batch number 309000\n",
      "Training mini-batch number 310000\n",
      "Training mini-batch number 311000\n",
      "Training mini-batch number 312000\n",
      "Training mini-batch number 313000\n",
      "Training mini-batch number 314000\n",
      "Training mini-batch number 315000\n",
      "Training mini-batch number 316000\n",
      "Training mini-batch number 317000\n",
      "Training mini-batch number 318000\n",
      "Training mini-batch number 319000\n",
      "Training mini-batch number 320000\n",
      "Training mini-batch number 321000\n",
      "Training mini-batch number 322000\n",
      "Training mini-batch number 323000\n",
      "Training mini-batch number 324000\n",
      "Epoch 12: validation accuracy 99.41%\n",
      "Training mini-batch number 325000\n",
      "Training mini-batch number 326000\n",
      "Training mini-batch number 327000\n",
      "Training mini-batch number 328000\n",
      "Training mini-batch number 329000\n",
      "Training mini-batch number 330000\n",
      "Training mini-batch number 331000\n",
      "Training mini-batch number 332000\n",
      "Training mini-batch number 333000\n",
      "Training mini-batch number 334000\n",
      "Training mini-batch number 335000\n",
      "Training mini-batch number 336000\n",
      "Training mini-batch number 337000\n",
      "Training mini-batch number 338000\n",
      "Training mini-batch number 339000\n",
      "Training mini-batch number 340000\n",
      "Training mini-batch number 341000\n",
      "Training mini-batch number 342000\n",
      "Training mini-batch number 343000\n",
      "Training mini-batch number 344000\n",
      "Training mini-batch number 345000\n",
      "Training mini-batch number 346000\n",
      "Training mini-batch number 347000\n",
      "Training mini-batch number 348000\n",
      "Training mini-batch number 349000\n",
      "Epoch 13: validation accuracy 99.35%\n",
      "Training mini-batch number 350000\n",
      "Training mini-batch number 351000\n",
      "Training mini-batch number 352000\n",
      "Training mini-batch number 353000\n",
      "Training mini-batch number 354000\n",
      "Training mini-batch number 355000\n",
      "Training mini-batch number 356000\n",
      "Training mini-batch number 357000\n",
      "Training mini-batch number 358000\n",
      "Training mini-batch number 359000\n",
      "Training mini-batch number 360000\n",
      "Training mini-batch number 361000\n",
      "Training mini-batch number 362000\n",
      "Training mini-batch number 363000\n",
      "Training mini-batch number 364000\n",
      "Training mini-batch number 365000\n",
      "Training mini-batch number 366000\n",
      "Training mini-batch number 367000\n",
      "Training mini-batch number 368000\n",
      "Training mini-batch number 369000\n",
      "Training mini-batch number 370000\n",
      "Training mini-batch number 371000\n",
      "Training mini-batch number 372000\n",
      "Training mini-batch number 373000\n",
      "Training mini-batch number 374000\n",
      "Epoch 14: validation accuracy 99.47%\n",
      "Training mini-batch number 375000\n",
      "Training mini-batch number 376000\n",
      "Training mini-batch number 377000\n",
      "Training mini-batch number 378000\n",
      "Training mini-batch number 379000\n",
      "Training mini-batch number 380000\n",
      "Training mini-batch number 381000\n",
      "Training mini-batch number 382000\n",
      "Training mini-batch number 383000\n",
      "Training mini-batch number 384000\n",
      "Training mini-batch number 385000\n",
      "Training mini-batch number 386000\n",
      "Training mini-batch number 387000\n",
      "Training mini-batch number 388000\n",
      "Training mini-batch number 389000\n",
      "Training mini-batch number 390000\n",
      "Training mini-batch number 391000\n",
      "Training mini-batch number 392000\n",
      "Training mini-batch number 393000\n",
      "Training mini-batch number 394000\n",
      "Training mini-batch number 395000\n",
      "Training mini-batch number 396000\n",
      "Training mini-batch number 397000\n",
      "Training mini-batch number 398000\n",
      "Training mini-batch number 399000\n",
      "Epoch 15: validation accuracy 99.45%\n",
      "Training mini-batch number 400000\n",
      "Training mini-batch number 401000\n",
      "Training mini-batch number 402000\n",
      "Training mini-batch number 403000\n",
      "Training mini-batch number 404000\n",
      "Training mini-batch number 405000\n",
      "Training mini-batch number 406000\n",
      "Training mini-batch number 407000\n",
      "Training mini-batch number 408000\n",
      "Training mini-batch number 409000\n",
      "Training mini-batch number 410000\n",
      "Training mini-batch number 411000\n",
      "Training mini-batch number 412000\n",
      "Training mini-batch number 413000\n",
      "Training mini-batch number 414000\n",
      "Training mini-batch number 415000\n",
      "Training mini-batch number 416000\n",
      "Training mini-batch number 417000\n",
      "Training mini-batch number 418000\n",
      "Training mini-batch number 419000\n",
      "Training mini-batch number 420000\n",
      "Training mini-batch number 421000\n",
      "Training mini-batch number 422000\n",
      "Training mini-batch number 423000\n",
      "Training mini-batch number 424000\n",
      "Epoch 16: validation accuracy 99.54%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.61%\n",
      "Training mini-batch number 425000\n",
      "Training mini-batch number 426000\n",
      "Training mini-batch number 427000\n",
      "Training mini-batch number 428000\n",
      "Training mini-batch number 429000\n",
      "Training mini-batch number 430000\n",
      "Training mini-batch number 431000\n",
      "Training mini-batch number 432000\n",
      "Training mini-batch number 433000\n",
      "Training mini-batch number 434000\n",
      "Training mini-batch number 435000\n",
      "Training mini-batch number 436000\n",
      "Training mini-batch number 437000\n",
      "Training mini-batch number 438000\n",
      "Training mini-batch number 439000\n",
      "Training mini-batch number 440000\n",
      "Training mini-batch number 441000\n",
      "Training mini-batch number 442000\n",
      "Training mini-batch number 443000\n",
      "Training mini-batch number 444000\n",
      "Training mini-batch number 445000\n",
      "Training mini-batch number 446000\n",
      "Training mini-batch number 447000\n",
      "Training mini-batch number 448000\n",
      "Training mini-batch number 449000\n",
      "Epoch 17: validation accuracy 99.47%\n",
      "Training mini-batch number 450000\n",
      "Training mini-batch number 451000\n",
      "Training mini-batch number 452000\n",
      "Training mini-batch number 453000\n",
      "Training mini-batch number 454000\n",
      "Training mini-batch number 455000\n",
      "Training mini-batch number 456000\n",
      "Training mini-batch number 457000\n",
      "Training mini-batch number 458000\n",
      "Training mini-batch number 459000\n",
      "Training mini-batch number 460000\n",
      "Training mini-batch number 461000\n",
      "Training mini-batch number 462000\n",
      "Training mini-batch number 463000\n",
      "Training mini-batch number 464000\n",
      "Training mini-batch number 465000\n",
      "Training mini-batch number 466000\n",
      "Training mini-batch number 467000\n",
      "Training mini-batch number 468000\n",
      "Training mini-batch number 469000\n",
      "Training mini-batch number 470000\n",
      "Training mini-batch number 471000\n",
      "Training mini-batch number 472000\n",
      "Training mini-batch number 473000\n",
      "Training mini-batch number 474000\n",
      "Epoch 18: validation accuracy 99.45%\n",
      "Training mini-batch number 475000\n",
      "Training mini-batch number 476000\n",
      "Training mini-batch number 477000\n",
      "Training mini-batch number 478000\n",
      "Training mini-batch number 479000\n",
      "Training mini-batch number 480000\n",
      "Training mini-batch number 481000\n",
      "Training mini-batch number 482000\n",
      "Training mini-batch number 483000\n",
      "Training mini-batch number 484000\n",
      "Training mini-batch number 485000\n",
      "Training mini-batch number 486000\n",
      "Training mini-batch number 487000\n",
      "Training mini-batch number 488000\n",
      "Training mini-batch number 489000\n",
      "Training mini-batch number 490000\n",
      "Training mini-batch number 491000\n",
      "Training mini-batch number 492000\n",
      "Training mini-batch number 493000\n",
      "Training mini-batch number 494000\n",
      "Training mini-batch number 495000\n",
      "Training mini-batch number 496000\n",
      "Training mini-batch number 497000\n",
      "Training mini-batch number 498000\n",
      "Training mini-batch number 499000\n",
      "Epoch 19: validation accuracy 99.47%\n",
      "Training mini-batch number 500000\n",
      "Training mini-batch number 501000\n",
      "Training mini-batch number 502000\n",
      "Training mini-batch number 503000\n",
      "Training mini-batch number 504000\n",
      "Training mini-batch number 505000\n",
      "Training mini-batch number 506000\n",
      "Training mini-batch number 507000\n",
      "Training mini-batch number 508000\n",
      "Training mini-batch number 509000\n",
      "Training mini-batch number 510000\n",
      "Training mini-batch number 511000\n",
      "Training mini-batch number 512000\n",
      "Training mini-batch number 513000\n",
      "Training mini-batch number 514000\n",
      "Training mini-batch number 515000\n",
      "Training mini-batch number 516000\n",
      "Training mini-batch number 517000\n",
      "Training mini-batch number 518000\n",
      "Training mini-batch number 519000\n",
      "Training mini-batch number 520000\n",
      "Training mini-batch number 521000\n",
      "Training mini-batch number 522000\n",
      "Training mini-batch number 523000\n",
      "Training mini-batch number 524000\n",
      "Epoch 20: validation accuracy 99.54%\n",
      "Training mini-batch number 525000\n",
      "Training mini-batch number 526000\n",
      "Training mini-batch number 527000\n",
      "Training mini-batch number 528000\n",
      "Training mini-batch number 529000\n",
      "Training mini-batch number 530000\n",
      "Training mini-batch number 531000\n",
      "Training mini-batch number 532000\n",
      "Training mini-batch number 533000\n",
      "Training mini-batch number 534000\n",
      "Training mini-batch number 535000\n",
      "Training mini-batch number 536000\n",
      "Training mini-batch number 537000\n",
      "Training mini-batch number 538000\n",
      "Training mini-batch number 539000\n",
      "Training mini-batch number 540000\n",
      "Training mini-batch number 541000\n",
      "Training mini-batch number 542000\n",
      "Training mini-batch number 543000\n",
      "Training mini-batch number 544000\n",
      "Training mini-batch number 545000\n",
      "Training mini-batch number 546000\n",
      "Training mini-batch number 547000\n",
      "Training mini-batch number 548000\n",
      "Training mini-batch number 549000\n",
      "Epoch 21: validation accuracy 99.50%\n",
      "Training mini-batch number 550000\n",
      "Training mini-batch number 551000\n",
      "Training mini-batch number 552000\n",
      "Training mini-batch number 553000\n",
      "Training mini-batch number 554000\n",
      "Training mini-batch number 555000\n",
      "Training mini-batch number 556000\n",
      "Training mini-batch number 557000\n",
      "Training mini-batch number 558000\n",
      "Training mini-batch number 559000\n",
      "Training mini-batch number 560000\n",
      "Training mini-batch number 561000\n",
      "Training mini-batch number 562000\n",
      "Training mini-batch number 563000\n",
      "Training mini-batch number 564000\n",
      "Training mini-batch number 565000\n",
      "Training mini-batch number 566000\n",
      "Training mini-batch number 567000\n",
      "Training mini-batch number 568000\n",
      "Training mini-batch number 569000\n",
      "Training mini-batch number 570000\n",
      "Training mini-batch number 571000\n",
      "Training mini-batch number 572000\n",
      "Training mini-batch number 573000\n",
      "Training mini-batch number 574000\n",
      "Epoch 22: validation accuracy 99.47%\n",
      "Training mini-batch number 575000\n",
      "Training mini-batch number 576000\n",
      "Training mini-batch number 577000\n",
      "Training mini-batch number 578000\n",
      "Training mini-batch number 579000\n",
      "Training mini-batch number 580000\n",
      "Training mini-batch number 581000\n",
      "Training mini-batch number 582000\n",
      "Training mini-batch number 583000\n",
      "Training mini-batch number 584000\n",
      "Training mini-batch number 585000\n",
      "Training mini-batch number 586000\n",
      "Training mini-batch number 587000\n",
      "Training mini-batch number 588000\n",
      "Training mini-batch number 589000\n",
      "Training mini-batch number 590000\n",
      "Training mini-batch number 591000\n",
      "Training mini-batch number 592000\n",
      "Training mini-batch number 593000\n",
      "Training mini-batch number 594000\n",
      "Training mini-batch number 595000\n",
      "Training mini-batch number 596000\n",
      "Training mini-batch number 597000\n",
      "Training mini-batch number 598000\n",
      "Training mini-batch number 599000\n",
      "Epoch 23: validation accuracy 99.52%\n",
      "Training mini-batch number 600000\n",
      "Training mini-batch number 601000\n",
      "Training mini-batch number 602000\n",
      "Training mini-batch number 603000\n",
      "Training mini-batch number 604000\n",
      "Training mini-batch number 605000\n",
      "Training mini-batch number 606000\n",
      "Training mini-batch number 607000\n",
      "Training mini-batch number 608000\n",
      "Training mini-batch number 609000\n",
      "Training mini-batch number 610000\n",
      "Training mini-batch number 611000\n",
      "Training mini-batch number 612000\n",
      "Training mini-batch number 613000\n",
      "Training mini-batch number 614000\n",
      "Training mini-batch number 615000\n",
      "Training mini-batch number 616000\n",
      "Training mini-batch number 617000\n",
      "Training mini-batch number 618000\n",
      "Training mini-batch number 619000\n",
      "Training mini-batch number 620000\n",
      "Training mini-batch number 621000\n",
      "Training mini-batch number 622000\n",
      "Training mini-batch number 623000\n",
      "Training mini-batch number 624000\n",
      "Epoch 24: validation accuracy 99.46%\n",
      "Training mini-batch number 625000\n",
      "Training mini-batch number 626000\n",
      "Training mini-batch number 627000\n",
      "Training mini-batch number 628000\n",
      "Training mini-batch number 629000\n",
      "Training mini-batch number 630000\n",
      "Training mini-batch number 631000\n",
      "Training mini-batch number 632000\n",
      "Training mini-batch number 633000\n",
      "Training mini-batch number 634000\n",
      "Training mini-batch number 635000\n",
      "Training mini-batch number 636000\n",
      "Training mini-batch number 637000\n",
      "Training mini-batch number 638000\n",
      "Training mini-batch number 639000\n",
      "Training mini-batch number 640000\n",
      "Training mini-batch number 641000\n",
      "Training mini-batch number 642000\n",
      "Training mini-batch number 643000\n",
      "Training mini-batch number 644000\n",
      "Training mini-batch number 645000\n",
      "Training mini-batch number 646000\n",
      "Training mini-batch number 647000\n",
      "Training mini-batch number 648000\n",
      "Training mini-batch number 649000\n",
      "Epoch 25: validation accuracy 99.57%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.56%\n",
      "Training mini-batch number 650000\n",
      "Training mini-batch number 651000\n",
      "Training mini-batch number 652000\n",
      "Training mini-batch number 653000\n",
      "Training mini-batch number 654000\n",
      "Training mini-batch number 655000\n",
      "Training mini-batch number 656000\n",
      "Training mini-batch number 657000\n",
      "Training mini-batch number 658000\n",
      "Training mini-batch number 659000\n",
      "Training mini-batch number 660000\n",
      "Training mini-batch number 661000\n",
      "Training mini-batch number 662000\n",
      "Training mini-batch number 663000\n",
      "Training mini-batch number 664000\n",
      "Training mini-batch number 665000\n",
      "Training mini-batch number 666000\n",
      "Training mini-batch number 667000\n",
      "Training mini-batch number 668000\n",
      "Training mini-batch number 669000\n",
      "Training mini-batch number 670000\n",
      "Training mini-batch number 671000\n",
      "Training mini-batch number 672000\n",
      "Training mini-batch number 673000\n",
      "Training mini-batch number 674000\n",
      "Epoch 26: validation accuracy 99.56%\n",
      "Training mini-batch number 675000\n",
      "Training mini-batch number 676000\n",
      "Training mini-batch number 677000\n",
      "Training mini-batch number 678000\n",
      "Training mini-batch number 679000\n",
      "Training mini-batch number 680000\n",
      "Training mini-batch number 681000\n",
      "Training mini-batch number 682000\n",
      "Training mini-batch number 683000\n",
      "Training mini-batch number 684000\n",
      "Training mini-batch number 685000\n",
      "Training mini-batch number 686000\n",
      "Training mini-batch number 687000\n",
      "Training mini-batch number 688000\n",
      "Training mini-batch number 689000\n",
      "Training mini-batch number 690000\n",
      "Training mini-batch number 691000\n",
      "Training mini-batch number 692000\n",
      "Training mini-batch number 693000\n",
      "Training mini-batch number 694000\n",
      "Training mini-batch number 695000\n",
      "Training mini-batch number 696000\n",
      "Training mini-batch number 697000\n",
      "Training mini-batch number 698000\n",
      "Training mini-batch number 699000\n",
      "Epoch 27: validation accuracy 99.53%\n",
      "Training mini-batch number 700000\n",
      "Training mini-batch number 701000\n",
      "Training mini-batch number 702000\n",
      "Training mini-batch number 703000\n",
      "Training mini-batch number 704000\n",
      "Training mini-batch number 705000\n",
      "Training mini-batch number 706000\n",
      "Training mini-batch number 707000\n",
      "Training mini-batch number 708000\n",
      "Training mini-batch number 709000\n",
      "Training mini-batch number 710000\n",
      "Training mini-batch number 711000\n",
      "Training mini-batch number 712000\n",
      "Training mini-batch number 713000\n",
      "Training mini-batch number 714000\n",
      "Training mini-batch number 715000\n",
      "Training mini-batch number 716000\n",
      "Training mini-batch number 717000\n",
      "Training mini-batch number 718000\n",
      "Training mini-batch number 719000\n",
      "Training mini-batch number 720000\n",
      "Training mini-batch number 721000\n",
      "Training mini-batch number 722000\n",
      "Training mini-batch number 723000\n",
      "Training mini-batch number 724000\n",
      "Epoch 28: validation accuracy 99.56%\n",
      "Training mini-batch number 725000\n",
      "Training mini-batch number 726000\n",
      "Training mini-batch number 727000\n",
      "Training mini-batch number 728000\n",
      "Training mini-batch number 729000\n",
      "Training mini-batch number 730000\n",
      "Training mini-batch number 731000\n",
      "Training mini-batch number 732000\n",
      "Training mini-batch number 733000\n",
      "Training mini-batch number 734000\n",
      "Training mini-batch number 735000\n",
      "Training mini-batch number 736000\n",
      "Training mini-batch number 737000\n",
      "Training mini-batch number 738000\n",
      "Training mini-batch number 739000\n",
      "Training mini-batch number 740000\n",
      "Training mini-batch number 741000\n",
      "Training mini-batch number 742000\n",
      "Training mini-batch number 743000\n",
      "Training mini-batch number 744000\n",
      "Training mini-batch number 745000\n",
      "Training mini-batch number 746000\n",
      "Training mini-batch number 747000\n",
      "Training mini-batch number 748000\n",
      "Training mini-batch number 749000\n",
      "Epoch 29: validation accuracy 99.60%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 99.61%\n",
      "Training mini-batch number 750000\n",
      "Training mini-batch number 751000\n",
      "Training mini-batch number 752000\n",
      "Training mini-batch number 753000\n",
      "Training mini-batch number 754000\n",
      "Training mini-batch number 755000\n",
      "Training mini-batch number 756000\n",
      "Training mini-batch number 757000\n",
      "Training mini-batch number 758000\n",
      "Training mini-batch number 759000\n",
      "Training mini-batch number 760000\n",
      "Training mini-batch number 761000\n",
      "Training mini-batch number 762000\n",
      "Training mini-batch number 763000\n",
      "Training mini-batch number 764000\n",
      "Training mini-batch number 765000\n",
      "Training mini-batch number 766000\n",
      "Training mini-batch number 767000\n",
      "Training mini-batch number 768000\n",
      "Training mini-batch number 769000\n",
      "Training mini-batch number 770000\n",
      "Training mini-batch number 771000\n",
      "Training mini-batch number 772000\n",
      "Training mini-batch number 773000\n",
      "Training mini-batch number 774000\n",
      "Epoch 30: validation accuracy 99.48%\n",
      "Training mini-batch number 775000\n",
      "Training mini-batch number 776000\n",
      "Training mini-batch number 777000\n",
      "Training mini-batch number 778000\n",
      "Training mini-batch number 779000\n",
      "Training mini-batch number 780000\n",
      "Training mini-batch number 781000\n",
      "Training mini-batch number 782000\n",
      "Training mini-batch number 783000\n",
      "Training mini-batch number 784000\n",
      "Training mini-batch number 785000\n",
      "Training mini-batch number 786000\n",
      "Training mini-batch number 787000\n",
      "Training mini-batch number 788000\n",
      "Training mini-batch number 789000\n",
      "Training mini-batch number 790000\n",
      "Training mini-batch number 791000\n",
      "Training mini-batch number 792000\n",
      "Training mini-batch number 793000\n",
      "Training mini-batch number 794000\n",
      "Training mini-batch number 795000\n",
      "Training mini-batch number 796000\n",
      "Training mini-batch number 797000\n",
      "Training mini-batch number 798000\n",
      "Training mini-batch number 799000\n",
      "Epoch 31: validation accuracy 99.55%\n",
      "Training mini-batch number 800000\n",
      "Training mini-batch number 801000\n",
      "Training mini-batch number 802000\n",
      "Training mini-batch number 803000\n",
      "Training mini-batch number 804000\n",
      "Training mini-batch number 805000\n",
      "Training mini-batch number 806000\n",
      "Training mini-batch number 807000\n",
      "Training mini-batch number 808000\n",
      "Training mini-batch number 809000\n",
      "Training mini-batch number 810000\n",
      "Training mini-batch number 811000\n",
      "Training mini-batch number 812000\n",
      "Training mini-batch number 813000\n",
      "Training mini-batch number 814000\n",
      "Training mini-batch number 815000\n",
      "Training mini-batch number 816000\n",
      "Training mini-batch number 817000\n",
      "Training mini-batch number 818000\n",
      "Training mini-batch number 819000\n",
      "Training mini-batch number 820000\n",
      "Training mini-batch number 821000\n",
      "Training mini-batch number 822000\n",
      "Training mini-batch number 823000\n",
      "Training mini-batch number 824000\n",
      "Epoch 32: validation accuracy 99.53%\n",
      "Training mini-batch number 825000\n",
      "Training mini-batch number 826000\n",
      "Training mini-batch number 827000\n",
      "Training mini-batch number 828000\n",
      "Training mini-batch number 829000\n",
      "Training mini-batch number 830000\n",
      "Training mini-batch number 831000\n",
      "Training mini-batch number 832000\n",
      "Training mini-batch number 833000\n",
      "Training mini-batch number 834000\n",
      "Training mini-batch number 835000\n",
      "Training mini-batch number 836000\n",
      "Training mini-batch number 837000\n",
      "Training mini-batch number 838000\n",
      "Training mini-batch number 839000\n",
      "Training mini-batch number 840000\n",
      "Training mini-batch number 841000\n",
      "Training mini-batch number 842000\n",
      "Training mini-batch number 843000\n",
      "Training mini-batch number 844000\n",
      "Training mini-batch number 845000\n",
      "Training mini-batch number 846000\n",
      "Training mini-batch number 847000\n",
      "Training mini-batch number 848000\n",
      "Training mini-batch number 849000\n",
      "Epoch 33: validation accuracy 99.50%\n",
      "Training mini-batch number 850000\n",
      "Training mini-batch number 851000\n",
      "Training mini-batch number 852000\n",
      "Training mini-batch number 853000\n",
      "Training mini-batch number 854000\n",
      "Training mini-batch number 855000\n",
      "Training mini-batch number 856000\n",
      "Training mini-batch number 857000\n",
      "Training mini-batch number 858000\n",
      "Training mini-batch number 859000\n",
      "Training mini-batch number 860000\n",
      "Training mini-batch number 861000\n",
      "Training mini-batch number 862000\n",
      "Training mini-batch number 863000\n",
      "Training mini-batch number 864000\n",
      "Training mini-batch number 865000\n",
      "Training mini-batch number 866000\n",
      "Training mini-batch number 867000\n",
      "Training mini-batch number 868000\n",
      "Training mini-batch number 869000\n",
      "Training mini-batch number 870000\n",
      "Training mini-batch number 871000\n",
      "Training mini-batch number 872000\n",
      "Training mini-batch number 873000\n",
      "Training mini-batch number 874000\n",
      "Epoch 34: validation accuracy 99.51%\n",
      "Training mini-batch number 875000\n",
      "Training mini-batch number 876000\n",
      "Training mini-batch number 877000\n",
      "Training mini-batch number 878000\n",
      "Training mini-batch number 879000\n",
      "Training mini-batch number 880000\n",
      "Training mini-batch number 881000\n",
      "Training mini-batch number 882000\n",
      "Training mini-batch number 883000\n",
      "Training mini-batch number 884000\n",
      "Training mini-batch number 885000\n",
      "Training mini-batch number 886000\n",
      "Training mini-batch number 887000\n",
      "Training mini-batch number 888000\n",
      "Training mini-batch number 889000\n",
      "Training mini-batch number 890000\n",
      "Training mini-batch number 891000\n",
      "Training mini-batch number 892000\n",
      "Training mini-batch number 893000\n",
      "Training mini-batch number 894000\n",
      "Training mini-batch number 895000\n",
      "Training mini-batch number 896000\n",
      "Training mini-batch number 897000\n",
      "Training mini-batch number 898000\n",
      "Training mini-batch number 899000\n",
      "Epoch 35: validation accuracy 99.49%\n",
      "Training mini-batch number 900000\n",
      "Training mini-batch number 901000\n",
      "Training mini-batch number 902000\n",
      "Training mini-batch number 903000\n",
      "Training mini-batch number 904000\n",
      "Training mini-batch number 905000\n",
      "Training mini-batch number 906000\n",
      "Training mini-batch number 907000\n",
      "Training mini-batch number 908000\n",
      "Training mini-batch number 909000\n",
      "Training mini-batch number 910000\n",
      "Training mini-batch number 911000\n",
      "Training mini-batch number 912000\n",
      "Training mini-batch number 913000\n",
      "Training mini-batch number 914000\n",
      "Training mini-batch number 915000\n",
      "Training mini-batch number 916000\n",
      "Training mini-batch number 917000\n",
      "Training mini-batch number 918000\n",
      "Training mini-batch number 919000\n",
      "Training mini-batch number 920000\n",
      "Training mini-batch number 921000\n",
      "Training mini-batch number 922000\n",
      "Training mini-batch number 923000\n",
      "Training mini-batch number 924000\n",
      "Epoch 36: validation accuracy 99.57%\n",
      "Training mini-batch number 925000\n",
      "Training mini-batch number 926000\n",
      "Training mini-batch number 927000\n",
      "Training mini-batch number 928000\n",
      "Training mini-batch number 929000\n",
      "Training mini-batch number 930000\n",
      "Training mini-batch number 931000\n",
      "Training mini-batch number 932000\n",
      "Training mini-batch number 933000\n",
      "Training mini-batch number 934000\n",
      "Training mini-batch number 935000\n",
      "Training mini-batch number 936000\n",
      "Training mini-batch number 937000\n",
      "Training mini-batch number 938000\n",
      "Training mini-batch number 939000\n",
      "Training mini-batch number 940000\n",
      "Training mini-batch number 941000\n",
      "Training mini-batch number 942000\n",
      "Training mini-batch number 943000\n",
      "Training mini-batch number 944000\n",
      "Training mini-batch number 945000\n",
      "Training mini-batch number 946000\n",
      "Training mini-batch number 947000\n",
      "Training mini-batch number 948000\n",
      "Training mini-batch number 949000\n",
      "Epoch 37: validation accuracy 99.59%\n",
      "Training mini-batch number 950000\n",
      "Training mini-batch number 951000\n",
      "Training mini-batch number 952000\n",
      "Training mini-batch number 953000\n",
      "Training mini-batch number 954000\n",
      "Training mini-batch number 955000\n",
      "Training mini-batch number 956000\n",
      "Training mini-batch number 957000\n",
      "Training mini-batch number 958000\n",
      "Training mini-batch number 959000\n",
      "Training mini-batch number 960000\n",
      "Training mini-batch number 961000\n",
      "Training mini-batch number 962000\n",
      "Training mini-batch number 963000\n",
      "Training mini-batch number 964000\n",
      "Training mini-batch number 965000\n",
      "Training mini-batch number 966000\n",
      "Training mini-batch number 967000\n",
      "Training mini-batch number 968000\n",
      "Training mini-batch number 969000\n",
      "Training mini-batch number 970000\n",
      "Training mini-batch number 971000\n",
      "Training mini-batch number 972000\n",
      "Training mini-batch number 973000\n",
      "Training mini-batch number 974000\n",
      "Epoch 38: validation accuracy 99.55%\n",
      "Training mini-batch number 975000\n",
      "Training mini-batch number 976000\n",
      "Training mini-batch number 977000\n",
      "Training mini-batch number 978000\n",
      "Training mini-batch number 979000\n",
      "Training mini-batch number 980000\n",
      "Training mini-batch number 981000\n",
      "Training mini-batch number 982000\n",
      "Training mini-batch number 983000\n",
      "Training mini-batch number 984000\n",
      "Training mini-batch number 985000\n",
      "Training mini-batch number 986000\n",
      "Training mini-batch number 987000\n",
      "Training mini-batch number 988000\n",
      "Training mini-batch number 989000\n",
      "Training mini-batch number 990000\n",
      "Training mini-batch number 991000\n",
      "Training mini-batch number 992000\n",
      "Training mini-batch number 993000\n",
      "Training mini-batch number 994000\n",
      "Training mini-batch number 995000\n",
      "Training mini-batch number 996000\n",
      "Training mini-batch number 997000\n",
      "Training mini-batch number 998000\n",
      "Training mini-batch number 999000\n",
      "Epoch 39: validation accuracy 99.59%\n",
      "Finished training network.\n",
      "Best validation accuracy of 99.60% obtained at iteration 749999\n",
      "Corresponding test accuracy of 99.61%\n"
     ]
    }
   ],
   "source": [
    "net.SGD(expanded_training_data, 40, mini_batch_size, 0.03, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

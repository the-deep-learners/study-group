# study-group

![Laura Graesser and Wah Loon Keng](https://github.com/the-deep-learners/study-group/blob/master/wiki-resources/IMG_9147.JPG)

We are a diverse group of software engineering and data science professionals from industry and academia. 

Inspired by the [collaborative culture of artificial neural network research](https://www.untapt.com/industry/2016/08/02/deep-learning-study-group/), we hold regular, chilled beverage-enhanced study sessions in midtown Manhattan. At these meetings, we summarise prescribed preparatory material and leverage our individual strengths in computer science, mathematics, statistics, neuroscience, and venture capital to cement our comprehension of concepts and to implement effective deep learning models. 

![smile and wave](https://github.com/the-deep-learners/study-group/blob/master/wiki-resources/session_XI_crew.JPG)

Over the course of our sessions, we follow three parallel paths: 

1. **Theory**: We study academic textbooks, exercises, and coursework so that we command strong theoretical foundations for neural networks and deep learning. Broadly, we cover calculus, algebra, probability, computer science, with a focus on their intersection at machine learning. 
2. **Application**: We practice deep learning in the real world. We typically commence by collectively following tutorials then we move on to solving novel and illustrative data problems involving a broad range of techniques. In addition to incorporating deep learning into our respective academic and commercial applications, we commit code to the present, public repository where possible. 
3. **Presentations**: Study group members regularly share their progress on Deep Learning projects and their area of expertise. This elicits novel discourse outside of the relatively formal paths **1** and **2**, playfully encouraging along serendipity. 

![jk1](https://github.com/the-deep-learners/study-group/blob/master/wiki-resources/3.jpg)


## Theory

Theory coverage has been led by:

* [Jon Krohn](https://www.jonkrohn.com/)
     * **Artificial Neural Networks** with Michael Nielsen's introductory text [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) (covered in *sessions I through V*)
     * **Machine Vision** with Fei-Fei Li, Andrej Karpathy and Justin Johnson's [CS231n on Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/) (*sessions VI through VIII*)
     * **Natural Language Processing** with Richard Socher and Christopher Manning's [CS224D (2016)](https://cs224d.stanford.edu/)/[CS224N (2017)](http://web.stanford.edu/class/cs224n/) on Deep Learning for Natural Language Processing (*sessions IX through XIII*)
     * **Reinforcement Learning** with Sergey Levine's [CS294-112 on Deep Reinforcement Learning](https://www.youtube.com/watch?v=8jQIKgTzQd4) (*session XIV*)
* [Laura Graesser](https://www.linkedin.com/in/laura-graesser-60006039/) and [Wah Loon Keng](https://www.linkedin.com/in/theoriesinpractice/) on **Deep Reinforcement Learning**
     * Deep Q-Learning with their [OpenAI Lab](https://github.com/kengz/openai_lab) (*session XV*)
     * Policy Gradients with their [SLM Lab](https://github.com/kengz/SLM-Lab/) (*session XVI*)

If you're looking to get a handle on the fundamentals of Deep Learning, check out Jon Krohn's:

* six-hour introduction to [Deep Learning in general](https://www.safaribooksonline.com/library/view/deep-learning-with/9780134770826/) 
* five-hour introduction to [Deep Learning for Natural Language Processing specifically](https://www.safaribooksonline.com/library/view/deep-learning-for/9780134851921/), or his
* thirty-hour, [in-classroom course](https://medium.com/@jjpkrohn/build-your-own-deep-learning-project-via-my-30-hour-course-e995ea025ba8) at the NYC Data Science Academy. 


## Application

Our applications have involved a broad range of neural network architectures built largely in Python with [Numpy](https://github.com/the-deep-learners/study-group/tree/master/nn-from-scratch), [TensorFlow, and/or Keras](https://insights.untapt.com/fundamental-deep-learning-code-in-tflearn-keras-theano-and-tensorflow-66be10a03227). 

![claudia](https://github.com/the-deep-learners/study-group/blob/master/wiki-resources/IMG_7641.JPG)

## Presentations

In chronological order, we have experienced the joy of being enlightened by: 

1. [Katya Vasilaky](https://kathrynthegreat.github.io/) on the mathematics of deep learning (*session II*) and on regularization (*session VIII*)
2. [Thomas Balestri](https://www.linkedin.com/in/thomasbalestri) on countless machine-learning underpinnings (*sessions III and IV*), LSTM gates (*session XII*), and Reinforcement Learning (*session XIV*)
3. [Gabe Rives-Corbett](https://www.linkedin.com/in/grivescorbett) on Keras implementations of deep learning deployed at [untapt](https://www.untapt.com/) (*session III*)
2. [Dmitri Nesterenko](https://github.com/dmitrinesterenko) on his NumPy implementation of *k*-Nearest Neighbours (*session VI*)
3. [Raphaela Sapire](https://www.linkedin.com/in/raphaelasapire) on the deep learning start-up investment atmosphere (*session VIII*)
3. [Grant Beyleveld](https://grantbeyleveld.wordpress.com/) on his U-Net Convolutional Network (*session IX*) and GAN (*session XII*) implementations 
3. [Jessica Graves](https://sefleuria.tumblr.com/) on applications of Deep Learning to the fashion industry (*session IX*)
4. [V.T. Rajan](https://www.linkedin.com/in/vtrajanphd/) on deriving the word2vec algorithm (*session X*)
5. Karl Habermas on his NumPy implemention of the word2vec algorithm (*session X*)
6. David Epstein on Generative Adversarial Networks (*session X*)
7. [Claudia Perlich](https://sites.google.com/site/claudiaperlich/home) on predictability and how it creates biases when your target is created by mixtures (*session XI*)
8. [Brian Dalessandro](https://www.linkedin.com/in/briandalessandro/) on [generating text with Keras LSTM models](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py) (*session XI*)
9. [Marianne Monteiro](https://www.linkedin.com/in/mariannelinharesm/) on TensorFlow Recurrent Neural Network implementations (*session XIII*)
10. [Druce Vertes](https://www.linkedin.com/in/drucevertes/) on predicting which financial stories go viral on social media (*session XIII*)

![katya](https://github.com/the-deep-learners/study-group/blob/master/wiki-resources/IMG_5974.JPG)

***

# Session Notes

Click through for detailed summary notes from each session: 

1. [August 17th, 2016](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week1): Perceptrons and Sigmoid Neurons
2. [September 6th, 2016](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week2): The Backpropagation Algorithm
3. [September 28th, 2016](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week3): Improving Neural Networks
4. [October 20th, 2016](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week4): Proofs of Key Properties
5. [November 10th, 2016](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week5): Deep (Conv)Nets
6. [November 30th, 2016](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week6): Convolutional Neural Networks for Visual Recognition
7. [January 12th, 2017](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week7): Implementing Convolutional Nets
8. [February 7th, 2017](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week8): Unsupervised Learning, Regularisation, and Venture Capital
9. [March 6th, 2017](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week9): Word Vectors, AI *x* Fashion, and U-Net
10. [March 27th, 2017](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week10): word2vec Mania + GANs
11. [April 19th, 2017](https://github.com/the-deep-learners/study-group/blob/master/weekly-work/week11): Recurrent Neural Networks, including GRUs and LSTMs
12. [July 1st, 2017](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week12): Translation, Attention, more LSTMs, Speech-to-Text, and TreeRNNs
13. [August 5th, 2017](https://github.com/the-deep-learners/study-group/blob/master/weekly-work/week13): Model Architectures for Answering Questions and Overcoming NLP Limits
14. [October 17th, 2017](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week14): Reinforcement Learning
15. [December 9th, 2017](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week15): Deep Reinforcement Learning (Deep Q-Learning and OpenAI Lab)
16. [February 17th, 2018](https://github.com/the-deep-learners/study-group/tree/master/weekly-work/week16): Deep Reinforcement Learning (Policy Gradients and SLM-Lab)

***

# Acknowledgements

![classic Ed Donner](https://github.com/the-deep-learners/study-group/blob/master/wiki-resources/5_cropped.jpg)

Thank you to [untapt](https://www.untapt.com/) and its visionary, neural net-loving CEO Ed Donner for hosting and subsidising all meetings of the Deep Learning Study Group. 

***


*With a desire to remain intimately-sized, our study group has reached its capacity. If you'd like to be added to our waiting list, please contact the organiser, [Jon Krohn](https://www.jonkrohn.com/contact/), describing your relevant experience as well as your interest in deep learning. We don't expect you to necessarily be a deep learning expert already :)*

![Jon Krohn](https://github.com/the-deep-learners/study-group/blob/master/wiki-resources/IMG_5959.JPG)

***
